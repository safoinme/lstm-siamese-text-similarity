{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hive-LSTM Siamese Integration Testing Notebook\n",
    "\n",
    "This notebook allows you to test the complete Hive data extraction, LSTM Siamese text similarity matching, and result storage workflow before deploying to Kubeflow.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Setup & Configuration** - Configure connections and parameters\n",
    "2. **Data Extraction** - Extract data from Hive table to CSV\n",
    "3. **Data Preprocessing** - Convert to LSTM Siamese format\n",
    "4. **Text Similarity Matching** - Run LSTM Siamese matching\n",
    "5. **Result Analysis** - Analyze matching results\n",
    "6. **Save to Hive** - Store results back to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append('.')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "HIVE_CONFIG = {\n",
    "    'host': '172.17.235.21',\n",
    "    'port': 10000,\n",
    "    'database': 'preprocessed_analytics',\n",
    "    'username': 'lhimer'\n",
    "}\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    'input_table': 'preprocessed_analytics.model_reference',\n",
    "    'output_table': 'results.lstm_siamese_matches', \n",
    "    'temp_dir': './temp_notebook_lstm',\n",
    "    'input_csv': './temp_notebook_lstm/input_data.csv',\n",
    "    'output_csv': './temp_notebook_lstm/output_results.csv',\n",
    "    'model_path': './temp_notebook_lstm/siamese_model.h5',\n",
    "    'sample_size': 1000,\n",
    "    'matching_mode': 'auto'\n",
    "}\n",
    "\n",
    "SIAMESE_CONFIG = {\n",
    "    'EMBEDDING_DIM': 300,\n",
    "    'MAX_SEQUENCE_LENGTH': 100,\n",
    "    'NUMBER_LSTM': 50,\n",
    "    'RATE_DROP_LSTM': 0.25,\n",
    "    'NUMBER_DENSE_UNITS': 50,\n",
    "    'ACTIVATION_FUNCTION': 'relu',\n",
    "    'RATE_DROP_DENSE': 0.25,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'EPOCHS': 10,\n",
    "    'BATCH_SIZE': 64\n",
    "}\n",
    "\n",
    "os.makedirs(DATA_CONFIG['temp_dir'], exist_ok=True)\n",
    "print(\"üìã Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data extraction\n",
    "print(\"üîÑ Testing data extraction...\")\n",
    "\n",
    "try:\n",
    "    from hive_siamese_data_extractor import HiveSiameseDataExtractor\n",
    "    \n",
    "    extractor = HiveSiameseDataExtractor(\n",
    "        host=HIVE_CONFIG['host'],\n",
    "        port=HIVE_CONFIG['port'],\n",
    "        username=HIVE_CONFIG['username'],\n",
    "        database=HIVE_CONFIG['database']\n",
    "    )\n",
    "    \n",
    "    if extractor.connect():\n",
    "        print(\"‚úÖ Hive connection successful!\")\n",
    "        \n",
    "        # Extract sample data\n",
    "        output_path = extractor.extract_and_convert(\n",
    "            table_name=DATA_CONFIG['input_table'],\n",
    "            output_path=DATA_CONFIG['input_csv'],\n",
    "            sample_limit=DATA_CONFIG['sample_size'],\n",
    "            matching_mode=DATA_CONFIG['matching_mode']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Data extracted to: {output_path}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        df = pd.read_csv(output_path)\n",
    "        print(f\"\\nüìä Sample data ({len(df)} rows):\")\n",
    "        display(df.head())\n",
    "        \n",
    "        extractor.disconnect()\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Hive connection failed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during extraction: {e}\")\n",
    "    print(\"üí° Creating sample data for testing...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        'sentences1': [\n",
    "            'John Smith works at Microsoft',\n",
    "            'Mary Johnson is a teacher',\n",
    "            'The quick brown fox jumps'\n",
    "        ],\n",
    "        'sentences2': [\n",
    "            'Jon Smith employed by Microsoft',\n",
    "            'Maria Johnson teaches students',\n",
    "            'A fast brown fox leaps'\n",
    "        ],\n",
    "        'is_similar': [1, 1, 1]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(DATA_CONFIG['input_csv'], index=False)\n",
    "    print(f\"‚úÖ Sample data created: {DATA_CONFIG['input_csv']}\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7", 
   "metadata": {},
   "source": [
    "## 4. Model Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LSTM Siamese model training\n",
    "print(\"üîÑ Testing LSTM Siamese model...\")\n",
    "\n",
    "try:\n",
    "    from siamese_matcher import SiameseMatcher\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(DATA_CONFIG['input_csv'])\n",
    "    sentences1 = df['sentences1'].tolist()\n",
    "    sentences2 = df['sentences2'].tolist()\n",
    "    labels = df['is_similar'].tolist()\n",
    "    \n",
    "    print(f\"üìä Training data: {len(sentences1)} pairs\")\n",
    "    \n",
    "    # Initialize matcher\n",
    "    matcher = SiameseMatcher(SIAMESE_CONFIG)\n",
    "    \n",
    "    # Train model (if enough data)\n",
    "    if len(sentences1) > 2:\n",
    "        print(\"üöÄ Training model...\")\n",
    "        history = matcher.train(sentences1, sentences2, labels)\n",
    "        \n",
    "        # Save model\n",
    "        matcher.save_model(DATA_CONFIG['model_path'])\n",
    "        print(f\"üíæ Model saved to: {DATA_CONFIG['model_path']}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"üîÆ Making predictions...\")\n",
    "        predictions = matcher.predict(sentences1, sentences2)\n",
    "        \n",
    "        # Save results\n",
    "        results_df = df.copy()\n",
    "        results_df['similarity_score'] = predictions\n",
    "        results_df['prediction'] = (predictions > 0.5).astype(int)\n",
    "        results_df['model_type'] = 'lstm_siamese'\n",
    "        \n",
    "        results_df.to_csv(DATA_CONFIG['output_csv'], index=False)\n",
    "        print(f\"üíæ Results saved to: {DATA_CONFIG['output_csv']}\")\n",
    "        \n",
    "        print(\"\\nüìä Results:\")\n",
    "        display(results_df)\n",
    "        \n",
    "        print(\"‚úÖ Model training and prediction completed!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Not enough data for training (need more than 2 pairs)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model training: {e}\")\n",
    "    print(\"üí° This might be expected with very small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if os.path.exists(DATA_CONFIG['output_csv']):\n",
    "    print(\"üìä Analyzing results...\")\n",
    "    \n",
    "    results_df = pd.read_csv(DATA_CONFIG['output_csv'])\n",
    "    \n",
    "    print(f\"\\nüìã Results Summary:\")\n",
    "    print(f\"  Total pairs: {len(results_df)}\")\n",
    "    \n",
    "    if 'similarity_score' in results_df.columns:\n",
    "        print(f\"  Average similarity: {results_df['similarity_score'].mean():.3f}\")\n",
    "        print(f\"  Max similarity: {results_df['similarity_score'].max():.3f}\")\n",
    "        print(f\"  Min similarity: {results_df['similarity_score'].min():.3f}\")\n",
    "    \n",
    "    if 'prediction' in results_df.columns:\n",
    "        matches = results_df['prediction'].sum()\n",
    "        print(f\"  Predicted matches: {matches}\")\n",
    "        print(f\"  Match rate: {matches/len(results_df):.2%}\")\n",
    "    \n",
    "    # Visualization\n",
    "    if 'similarity_score' in results_df.columns and len(results_df) > 1:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(results_df['similarity_score'], bins=10, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Similarity Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Similarity Scores')\n",
    "        \n",
    "        if 'prediction' in results_df.columns:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            matches = results_df['prediction'].sum()\n",
    "            non_matches = len(results_df) - matches\n",
    "            plt.pie([matches, non_matches], labels=['Matches', 'Non-matches'], \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "            plt.title('Match Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nüìã Sample Results:\")\n",
    "    display(results_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate workflow summary\n",
    "print(\"üìã Generating workflow summary...\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_type': 'lstm_siamese',\n",
    "    'configuration': {\n",
    "        'hive_table': DATA_CONFIG['input_table'],\n",
    "        'output_table': DATA_CONFIG['output_table'],\n",
    "        'embedding_dim': SIAMESE_CONFIG['EMBEDDING_DIM'],\n",
    "        'max_sequence_length': SIAMESE_CONFIG['MAX_SEQUENCE_LENGTH'],\n",
    "        'sample_size': DATA_CONFIG['sample_size']\n",
    "    },\n",
    "    'files_created': [],\n",
    "    'status': 'completed'\n",
    "}\n",
    "\n",
    "# Check which files were created\n",
    "files_to_check = [\n",
    "    DATA_CONFIG['input_csv'],\n",
    "    DATA_CONFIG['output_csv'],\n",
    "    DATA_CONFIG['model_path']\n",
    "]\n",
    "\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        summary['files_created'].append(file_path)\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(DATA_CONFIG['temp_dir'], 'workflow_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\nüéâ LSTM Siamese Workflow Testing Complete!\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. ‚úÖ Test completed - ready for production deployment\")\n",
    "print(\"2. üîß Configure Kubeflow pipeline parameters\")\n",
    "print(\"3. üê≥ Build Docker image\")\n",
    "print(\"4. üöÄ Deploy to Kubeflow\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "for file_path in summary['files_created']:\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  ‚úÖ {file_path} ({size} bytes)\")\n",
    "\n",
    "print(\"\\nüìä Configuration Summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}