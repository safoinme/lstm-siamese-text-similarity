{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hive-LSTM Siamese Integration Testing Notebook\n",
    "\n",
    "This notebook allows you to test the complete Hive data extraction, LSTM Siamese text similarity matching, and result storage workflow before deploying to Kubeflow.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Setup & Configuration** - Configure connections and parameters\n",
    "2. **Data Extraction** - Extract data from Hive table to CSV\n",
    "3. **Data Preprocessing** - Convert to LSTM Siamese format\n",
    "4. **Text Similarity Matching** - Run LSTM Siamese matching\n",
    "5. **Result Analysis** - Analyze matching results\n",
    "6. **Save to Hive** - Store results back to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add current directory to path for local imports\n",
    "sys.path.append('.')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "HIVE_CONFIG = {\n",
    "    'host': '172.17.235.21',\n",
    "    'port': 10000,\n",
    "    'database': 'preprocessed_analytics',\n",
    "    'username': 'lhimer'\n",
    "}\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    'input_table': 'preprocessed_analytics.model_reference',\n",
    "    'output_table': 'results.lstm_siamese_matches', \n",
    "    'temp_dir': './temp_notebook_lstm',\n",
    "    'input_csv': './temp_notebook_lstm/input_data.csv',\n",
    "    'output_csv': './temp_notebook_lstm/output_results.csv',\n",
    "    'model_path': './temp_notebook_lstm/siamese_model.h5',\n",
    "    'sample_size': 1000,\n",
    "    'matching_mode': 'auto'\n",
    "}\n",
    "\n",
    "SIAMESE_CONFIG = {\n",
    "    'EMBEDDING_DIM': 300,\n",
    "    'MAX_SEQUENCE_LENGTH': 100,\n",
    "    'NUMBER_LSTM': 50,\n",
    "    'RATE_DROP_LSTM': 0.25,\n",
    "    'NUMBER_DENSE_UNITS': 50,\n",
    "    'ACTIVATION_FUNCTION': 'relu',\n",
    "    'RATE_DROP_DENSE': 0.25,\n",
    "    'VALIDATION_SPLIT': 0.2,\n",
    "    'EPOCHS': 10,\n",
    "    'BATCH_SIZE': 64\n",
    "}\n",
    "\n",
    "os.makedirs(DATA_CONFIG['temp_dir'], exist_ok=True)\n",
    "print(\"📋 Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data extraction\n",
    "print(\"🔄 Testing data extraction...\")\n",
    "\n",
    "try:\n",
    "    from hive_siamese_data_extractor import HiveSiameseDataExtractor\n",
    "    \n",
    "    extractor = HiveSiameseDataExtractor(\n",
    "        host=HIVE_CONFIG['host'],\n",
    "        port=HIVE_CONFIG['port'],\n",
    "        username=HIVE_CONFIG['username'],\n",
    "        database=HIVE_CONFIG['database']\n",
    "    )\n",
    "    \n",
    "    if extractor.connect():\n",
    "        print(\"✅ Hive connection successful!\")\n",
    "        \n",
    "        # Extract sample data\n",
    "        output_path = extractor.extract_and_convert(\n",
    "            table_name=DATA_CONFIG['input_table'],\n",
    "            output_path=DATA_CONFIG['input_csv'],\n",
    "            sample_limit=DATA_CONFIG['sample_size'],\n",
    "            matching_mode=DATA_CONFIG['matching_mode']\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Data extracted to: {output_path}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        df = pd.read_csv(output_path)\n",
    "        print(f\"\\n📊 Sample data ({len(df)} rows):\")\n",
    "        display(df.head())\n",
    "        \n",
    "        extractor.disconnect()\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Hive connection failed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during extraction: {e}\")\n",
    "    print(\"💡 Creating sample data for testing...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        'sentences1': [\n",
    "            'John Smith works at Microsoft',\n",
    "            'Mary Johnson is a teacher',\n",
    "            'The quick brown fox jumps'\n",
    "        ],\n",
    "        'sentences2': [\n",
    "            'Jon Smith employed by Microsoft',\n",
    "            'Maria Johnson teaches students',\n",
    "            'A fast brown fox leaps'\n",
    "        ],\n",
    "        'is_similar': [1, 1, 1]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(DATA_CONFIG['input_csv'], index=False)\n",
    "    print(f\"✅ Sample data created: {DATA_CONFIG['input_csv']}\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7", 
   "metadata": {},
   "source": [
    "## 4. Model Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LSTM Siamese model training\n",
    "print(\"🔄 Testing LSTM Siamese model...\")\n",
    "\n",
    "try:\n",
    "    from siamese_matcher import SiameseMatcher\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(DATA_CONFIG['input_csv'])\n",
    "    sentences1 = df['sentences1'].tolist()\n",
    "    sentences2 = df['sentences2'].tolist()\n",
    "    labels = df['is_similar'].tolist()\n",
    "    \n",
    "    print(f\"📊 Training data: {len(sentences1)} pairs\")\n",
    "    \n",
    "    # Initialize matcher\n",
    "    matcher = SiameseMatcher(SIAMESE_CONFIG)\n",
    "    \n",
    "    # Train model (if enough data)\n",
    "    if len(sentences1) > 2:\n",
    "        print(\"🚀 Training model...\")\n",
    "        history = matcher.train(sentences1, sentences2, labels)\n",
    "        \n",
    "        # Save model\n",
    "        matcher.save_model(DATA_CONFIG['model_path'])\n",
    "        print(f\"💾 Model saved to: {DATA_CONFIG['model_path']}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"🔮 Making predictions...\")\n",
    "        predictions = matcher.predict(sentences1, sentences2)\n",
    "        \n",
    "        # Save results\n",
    "        results_df = df.copy()\n",
    "        results_df['similarity_score'] = predictions\n",
    "        results_df['prediction'] = (predictions > 0.5).astype(int)\n",
    "        results_df['model_type'] = 'lstm_siamese'\n",
    "        \n",
    "        results_df.to_csv(DATA_CONFIG['output_csv'], index=False)\n",
    "        print(f\"💾 Results saved to: {DATA_CONFIG['output_csv']}\")\n",
    "        \n",
    "        print(\"\\n📊 Results:\")\n",
    "        display(results_df)\n",
    "        \n",
    "        print(\"✅ Model training and prediction completed!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  Not enough data for training (need more than 2 pairs)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during model training: {e}\")\n",
    "    print(\"💡 This might be expected with very small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if os.path.exists(DATA_CONFIG['output_csv']):\n",
    "    print(\"📊 Analyzing results...\")\n",
    "    \n",
    "    results_df = pd.read_csv(DATA_CONFIG['output_csv'])\n",
    "    \n",
    "    print(f\"\\n📋 Results Summary:\")\n",
    "    print(f\"  Total pairs: {len(results_df)}\")\n",
    "    \n",
    "    if 'similarity_score' in results_df.columns:\n",
    "        print(f\"  Average similarity: {results_df['similarity_score'].mean():.3f}\")\n",
    "        print(f\"  Max similarity: {results_df['similarity_score'].max():.3f}\")\n",
    "        print(f\"  Min similarity: {results_df['similarity_score'].min():.3f}\")\n",
    "    \n",
    "    if 'prediction' in results_df.columns:\n",
    "        matches = results_df['prediction'].sum()\n",
    "        print(f\"  Predicted matches: {matches}\")\n",
    "        print(f\"  Match rate: {matches/len(results_df):.2%}\")\n",
    "    \n",
    "    # Visualization\n",
    "    if 'similarity_score' in results_df.columns and len(results_df) > 1:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(results_df['similarity_score'], bins=10, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Similarity Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Similarity Scores')\n",
    "        \n",
    "        if 'prediction' in results_df.columns:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            matches = results_df['prediction'].sum()\n",
    "            non_matches = len(results_df) - matches\n",
    "            plt.pie([matches, non_matches], labels=['Matches', 'Non-matches'], \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "            plt.title('Match Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n📋 Sample Results:\")\n",
    "    display(results_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate workflow summary\n",
    "print(\"📋 Generating workflow summary...\")\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_type': 'lstm_siamese',\n",
    "    'configuration': {\n",
    "        'hive_table': DATA_CONFIG['input_table'],\n",
    "        'output_table': DATA_CONFIG['output_table'],\n",
    "        'embedding_dim': SIAMESE_CONFIG['EMBEDDING_DIM'],\n",
    "        'max_sequence_length': SIAMESE_CONFIG['MAX_SEQUENCE_LENGTH'],\n",
    "        'sample_size': DATA_CONFIG['sample_size']\n",
    "    },\n",
    "    'files_created': [],\n",
    "    'status': 'completed'\n",
    "}\n",
    "\n",
    "# Check which files were created\n",
    "files_to_check = [\n",
    "    DATA_CONFIG['input_csv'],\n",
    "    DATA_CONFIG['output_csv'],\n",
    "    DATA_CONFIG['model_path']\n",
    "]\n",
    "\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        summary['files_created'].append(file_path)\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(DATA_CONFIG['temp_dir'], 'workflow_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"💾 Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n🎉 LSTM Siamese Workflow Testing Complete!\")\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"1. ✅ Test completed - ready for production deployment\")\n",
    "print(\"2. 🔧 Configure Kubeflow pipeline parameters\")\n",
    "print(\"3. 🐳 Build Docker image\")\n",
    "print(\"4. 🚀 Deploy to Kubeflow\")\n",
    "\n",
    "print(f\"\\n📁 Generated Files:\")\n",
    "for file_path in summary['files_created']:\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  ✅ {file_path} ({size} bytes)\")\n",
    "\n",
    "print(\"\\n📊 Configuration Summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}