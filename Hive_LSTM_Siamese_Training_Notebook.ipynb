{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Siamese Model Training Notebook\n",
    "\n",
    "This notebook focuses on **training** the LSTM Siamese model with detailed steps. The trained model will then be used for inference in the Kubeflow pipeline.\n",
    "\n",
    "## Training Workflow:\n",
    "1. **Data Preparation** - Extract and prepare training data from Hive\n",
    "2. **Exploratory Data Analysis** - Understand data distribution and quality\n",
    "3. **Text Preprocessing** - Clean and tokenize text data\n",
    "4. **Model Architecture Design** - Configure LSTM Siamese architecture\n",
    "5. **Training Process** - Train model with monitoring and validation\n",
    "6. **Model Evaluation** - Assess performance and tune hyperparameters\n",
    "7. **Model Export** - Save trained model for production inference\n",
    "8. **Training Documentation** - Generate training report and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:23:37.127508: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 18:23:37.177746: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Information:\n",
      "  TensorFlow version: 2.13.0\n",
      "  GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "  CPU cores: 8\n",
      "‚úÖ Environment setup completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:23:39.364845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-23 18:23:39.371210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-23 18:23:39.371354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Dropout, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('.')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"  TensorFlow version: {tf.__version__}\")\n",
    "print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"  CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "print(\"‚úÖ Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Configuration:\n",
      "  Model: LSTM(128) + Dense(64)\n",
      "  Training samples: 10000\n",
      "  Epochs: 50\n",
      "  Batch size: 128\n",
      "  Sequence length: 100\n",
      "  Output directory: ./trained_models\n",
      "‚úÖ Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# Hive Configuration\n",
    "HIVE_CONFIG = {\n",
    "    'host': '172.17.235.21',\n",
    "    'port': 10000,\n",
    "    'database': 'preprocessed_analytics',\n",
    "    'username': 'lhimer'\n",
    "}\n",
    "\n",
    "# Data Configuration\n",
    "DATA_CONFIG = {\n",
    "    'input_table': 'preprocessed_analytics.model_reference',\n",
    "    'temp_dir': './training_data',\n",
    "    'raw_data_path': './training_data/raw_data.csv',\n",
    "    'processed_data_path': './training_data/processed_data.csv',\n",
    "    'training_data_path': './training_data/training_pairs.csv',\n",
    "    'validation_data_path': './training_data/validation_pairs.csv',\n",
    "    'test_data_path': './training_data/test_pairs.csv',\n",
    "    'sample_size': 10000,  # Larger sample for training\n",
    "    'matching_mode': 'cross_product',  # Generate training pairs\n",
    "    'balance_ratio': 0.5  # 50% similar, 50% dissimilar\n",
    "}\n",
    "\n",
    "# Model Architecture Configuration\n",
    "MODEL_CONFIG = {\n",
    "    'embedding_dim': 300,\n",
    "    'max_sequence_length': 100,\n",
    "    'number_lstm': 128,  # Larger for better capacity\n",
    "    'rate_drop_lstm': 0.3,\n",
    "    'number_dense_units': 64,\n",
    "    'activation_function': 'relu',\n",
    "    'rate_drop_dense': 0.3,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'epochs': 50,  # More epochs for thorough training\n",
    "    'batch_size': 128,\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'min_learning_rate': 1e-7\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_CONFIG = {\n",
    "    'model_dir': './trained_models',\n",
    "    'model_name': 'lstm_siamese_model',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'save_best_only': True,\n",
    "    'save_weights_only': False\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_CONFIG['temp_dir'], OUTPUT_CONFIG['model_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"  Model: LSTM({MODEL_CONFIG['number_lstm']}) + Dense({MODEL_CONFIG['number_dense_units']})\")\n",
    "print(f\"  Training samples: {DATA_CONFIG['sample_size']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  Sequence length: {MODEL_CONFIG['max_sequence_length']}\")\n",
    "print(f\"  Output directory: {OUTPUT_CONFIG['model_dir']}\")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "import os\nimport json\nimport argparse\nfrom typing import Dict, List, Optional, Tuple\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nfrom pyhive import hive\nfrom thrift.transport import TSocket\nfrom thrift.transport import TTransport\nfrom thrift.protocol import TBinaryProtocol\nimport random\nimport string\n\n\nclass HiveSiameseDataExtractor:\n    \"\"\"\n    Class to extract data from Hive tables and prepare for LSTM Siamese text similarity pipeline.\n    \"\"\"\n    \n    def __init__(self, host: str, port: int = 10000, username: str = None, database: str = \"default\"):\n        \"\"\"Initialize Hive connection parameters.\"\"\"\n        self.host = host\n        self.port = port\n        self.username = username\n        self.database = database\n        self.connection = None\n    \n    def connect(self):\n        \"\"\"Establish connection to Hive.\"\"\"\n        try:\n            self.connection = hive.Connection(\n                host=self.host,\n                port=self.port,\n                database=self.database,\n                username=self.username\n            )\n            print(f\"‚úÖ Connected to Hive: {self.host}:{self.port}/{self.database}\")\n            return True\n        except Exception as e:\n            print(f\"‚ùå Failed to connect to Hive: {e}\")\n            return False\n    \n    def disconnect(self):\n        \"\"\"Close Hive connection.\"\"\"\n        if self.connection:\n            self.connection.close()\n            print(\"üîå Hive connection closed\")\n    \n    def execute_query(self, query: str) -> pd.DataFrame:\n        \"\"\"Execute SQL query and return results as DataFrame.\"\"\"\n        if not self.connection:\n            raise RuntimeError(\"Not connected to Hive\")\n        \n        try:\n            df = pd.read_sql(query, self.connection)\n            print(f\"üìä Query executed successfully, returned {len(df)} rows\")\n            return df\n        except Exception as e:\n            print(f\"‚ùå Query failed: {e}\")\n            raise\n    \n    def add_random_char(self, text):\n        \"\"\"Add a random letter to the text at a random position\"\"\"\n        if pd.isna(text) or text == '':\n            return text\n        text = str(text)\n        pos = random.randint(0, len(text))\n        char = random.choice(string.ascii_uppercase)\n        return text[:pos] + char + text[pos:]\n    \n    def remove_random_chars(self, text, num_chars=1):\n        \"\"\"Remove 1-2 random characters from the text\"\"\"\n        if pd.isna(text) or text == '' or len(str(text)) <= num_chars:\n            return text\n        text = str(text)\n        for _ in range(min(num_chars, len(text))):\n            if len(text) > 1:\n                pos = random.randint(0, len(text) - 1)\n                text = text[:pos] + text[pos + 1:]\n        return text\n    \n    def change_random_chars(self, text, num_chars=1):\n        \"\"\"Change 1-2 random characters in the text\"\"\"\n        if pd.isna(text) or text == '':\n            return text\n        text = str(text)\n        text_list = list(text)\n        positions = random.sample(range(len(text_list)), min(num_chars, len(text_list)))\n        \n        for pos in positions:\n            if text_list[pos].isalpha():\n                text_list[pos] = random.choice(string.ascii_uppercase)\n            elif text_list[pos].isdigit():\n                text_list[pos] = random.choice(string.digits)\n        \n        return ''.join(text_list)\n    \n    def augment_field(self, text):\n        \"\"\"Apply one of the augmentation operations to a field\"\"\"\n        operations = [\n            lambda x: self.add_random_char(x),\n            lambda x: self.remove_random_chars(x, 1),\n            lambda x: self.remove_random_chars(x, 2),\n            lambda x: self.change_random_chars(x, 1),\n            lambda x: self.change_random_chars(x, 2)\n        ]\n        \n        operation = random.choice(operations)\n        return operation(text)\n    \n    def augment_row(self, row):\n        \"\"\"Augment a single row by modifying 1-2 fields\"\"\"\n        new_row = row.copy()\n        \n        # Get field names excluding empty ones\n        non_empty_fields = [col for col in row.index \n                          if not pd.isna(row[col]) and str(row[col]).strip() != '']\n        \n        if len(non_empty_fields) == 0:\n            return new_row\n        \n        # Randomly choose 1 or 2 fields to modify\n        num_fields_to_modify = random.choice([1, 2])\n        num_fields_to_modify = min(num_fields_to_modify, len(non_empty_fields))\n        \n        fields_to_modify = random.sample(non_empty_fields, num_fields_to_modify)\n        \n        for field in fields_to_modify:\n            new_row[field] = self.augment_field(row[field])\n        \n        return new_row\n    \n    def row_to_natural_sentence(self, row, df_columns):\n        \"\"\"Convert a row to natural language sentence format\"\"\"\n        sentence_parts = []\n        \n        for col in df_columns:\n            if pd.notna(row[col]) and str(row[col]).strip():\n                # Clean column name - remove table prefixes and make readable\n                clean_col = col.split('.', 1)[1] if '.' in col else col\n                clean_col = clean_col.replace('_', ' ')\n                \n                value = str(row[col]).strip()\n                sentence_parts.append(f\"my {clean_col} is {value}\")\n        \n        return \" and \".join(sentence_parts)\n    \n    def detect_table_structure(self, df: pd.DataFrame) -> Dict[str, any]:\n        \"\"\"\n        Detect if table has _left/_right columns (production) or single columns (testing).\n        \n        Args:\n            df: DataFrame to analyze\n            \n        Returns:\n            Dictionary with structure information\n        \"\"\"\n        columns = list(df.columns)\n        \n        # Remove table prefixes first to analyze column structure\n        clean_columns = []\n        for col in columns:\n            if '.' in col:\n                clean_col = col.split('.', 1)[1]\n            else:\n                clean_col = col\n            clean_columns.append(clean_col)\n        \n        # Check for _left and _right patterns\n        left_columns = [col for col in clean_columns if col.endswith('_left')]\n        right_columns = [col for col in clean_columns if col.endswith('_right')]\n        \n        # Extract base field names\n        left_fields = {col[:-5] for col in left_columns}  # Remove '_left'\n        right_fields = {col[:-6] for col in right_columns}  # Remove '_right'\n        \n        # Check if we have matching left/right pairs\n        matching_fields = left_fields.intersection(right_fields)\n        \n        if matching_fields:\n            structure_type = \"production\"\n            message = f\"üè≠ Production table detected with {len(matching_fields)} matching field pairs\"\n        else:\n            structure_type = \"testing\"\n            message = f\"üß™ Testing table detected with {len(clean_columns)} fields for training data generation\"\n        \n        return {\n            'type': structure_type,\n            'columns': columns,\n            'clean_columns': clean_columns,\n            'left_columns': left_columns,\n            'right_columns': right_columns,\n            'matching_fields': list(matching_fields),\n            'message': message\n        }\n    \n    def convert_production_format(self, df: pd.DataFrame, structure: Dict) -> pd.DataFrame:\n        \"\"\"Convert production table with _left/_right columns to LSTM Siamese format.\"\"\"\n        records = []\n        \n        for idx, row in df.iterrows():\n            # Build left and right sentences\n            left_parts = []\n            right_parts = []\n            \n            for field in structure['matching_fields']:\n                # Find the actual column names (with potential table prefixes)\n                left_col = None\n                right_col = None\n                \n                for col in df.columns:\n                    clean_col = col.split('.', 1)[1] if '.' in col else col\n                    if clean_col == f\"{field}_left\":\n                        left_col = col\n                    elif clean_col == f\"{field}_right\":\n                        right_col = col\n                \n                # Process left column\n                if left_col and pd.notna(row[left_col]) and str(row[left_col]).strip():\n                    value = str(row[left_col]).strip()\n                    left_parts.append(f\"my {field} is {value}\")\n                \n                # Process right column\n                if right_col and pd.notna(row[right_col]) and str(row[right_col]).strip():\n                    value = str(row[right_col]).strip()\n                    right_parts.append(f\"my {field} is {value}\")\n            \n            left_text = \" and \".join(left_parts)\n            right_text = \" and \".join(right_parts)\n            \n            # For production, determine similarity\n            # This could be based on exact match, fuzzy match, or existing label column\n            is_similar = 1 if left_text.lower().strip() == right_text.lower().strip() else 0\n            \n            # Check if there's an existing similarity/match column\n            similarity_cols = [col for col in df.columns if any(sim_term in col.lower() \n                             for sim_term in ['similar', 'match', 'label', 'ground_truth'])]\n            if similarity_cols:\n                # Use existing ground truth if available\n                is_similar = int(row[similarity_cols[0]]) if pd.notna(row[similarity_cols[0]]) else is_similar\n            \n            record = {\n                'sentences1': left_text,\n                'sentences2': right_text,\n                'is_similar': is_similar\n            }\n            records.append(record)\n        \n        result_df = pd.DataFrame(records)\n        print(f\"‚úÖ Successfully converted {len(records)} production records with left/right pairs\")\n        return result_df\n    \n    def convert_testing_format(self, df: pd.DataFrame, structure: Dict) -> pd.DataFrame:\n        \"\"\"Convert testing table for self-matching to LSTM Siamese format.\"\"\"\n        records = []\n        \n        for idx, row in df.iterrows():\n            # Convert row to natural language sentence\n            record_text = self.row_to_natural_sentence(row, df.columns)\n            \n            # Create record for self-matching (similar to itself)\n            record = {\n                'sentences1': record_text,\n                'sentences2': record_text,  # Same record for self-matching\n                'is_similar': 1  # Same record is always similar\n            }\n            records.append(record)\n        \n        result_df = pd.DataFrame(records)\n        print(f\"‚úÖ Successfully converted {len(records)} testing records for self-matching\")\n        return result_df\n    \n    def create_text_pairs(self, df: pd.DataFrame, mode: str = 'auto') -> pd.DataFrame:\n        \"\"\"\n        Create text pairs for LSTM Siamese training from different sources.\n        \n        Args:\n            df: Input DataFrame\n            mode: 'auto', 'production', 'testing', or 'cross_product'\n            \n        Returns:\n            DataFrame with sentences1, sentences2, is_similar columns\n        \"\"\"\n        if mode == 'cross_product':\n            return self.create_cross_product_pairs(df)\n        \n        # Detect table structure\n        structure = self.detect_table_structure(df)\n        print(structure['message'])\n        \n        if structure['type'] == 'production' and mode != 'testing':\n            return self.convert_production_format(df, structure)\n        else:\n            return self.convert_testing_format(df, structure)\n    \n    def create_cross_product_pairs(self, df: pd.DataFrame, max_pairs: int = 10000, augmentations_per_row: int = 3) -> pd.DataFrame:\n        \"\"\"\n        Create training pairs using augmentation strategy:\n        1. For each original row, create augmented versions\n        2. Pair original with augmented (similar pairs)\n        3. Pair different rows (dissimilar pairs)\n        4. Balance the dataset\n        \n        Args:\n            df: Input DataFrame\n            max_pairs: Maximum number of pairs to generate\n            augmentations_per_row: Number of augmented versions per row\n            \n        Returns:\n            DataFrame with sentence pairs and similarity labels\n        \"\"\"\n        print(f\"üîÑ Creating training pairs with augmentation strategy...\")\n        print(f\"  Original rows: {len(df)}\")\n        print(f\"  Augmentations per row: {augmentations_per_row}\")\n        \n        # Convert original rows to sentences\n        original_sentences = []\n        for idx, row in df.iterrows():\n            sentence = self.row_to_natural_sentence(row, df.columns)\n            original_sentences.append(sentence)\n        \n        # Generate augmented sentences\n        augmented_data = []\n        for idx, row in df.iterrows():\n            original_sentence = self.row_to_natural_sentence(row, df.columns)\n            \n            # Create augmented versions\n            for aug_num in range(augmentations_per_row):\n                augmented_row = self.augment_row(row)\n                augmented_sentence = self.row_to_natural_sentence(augmented_row, df.columns)\n                augmented_data.append({\n                    'original_idx': idx,\n                    'original_sentence': original_sentence,\n                    'augmented_sentence': augmented_sentence\n                })\n        \n        print(f\"  Generated {len(augmented_data)} augmented sentences\")\n        \n        # Create pairs\n        records = []\n        \n        # 1. Similar pairs: original vs augmented\n        similar_pairs = 0\n        for aug_data in augmented_data:\n            if len(records) >= max_pairs // 2:  # Reserve half for similar pairs\n                break\n            \n            record = {\n                'sentences1': aug_data['original_sentence'],\n                'sentences2': aug_data['augmented_sentence'],\n                'is_similar': 1\n            }\n            records.append(record)\n            similar_pairs += 1\n        \n        print(f\"  Created {similar_pairs} similar pairs (original vs augmented)\")\n        \n        # 2. Dissimilar pairs: different original sentences\n        dissimilar_pairs = 0\n        target_dissimilar = min(similar_pairs, max_pairs - similar_pairs)  # Balance with similar pairs\n        \n        attempts = 0\n        max_attempts = target_dissimilar * 10  # Avoid infinite loop\n        \n        while dissimilar_pairs < target_dissimilar and attempts < max_attempts:\n            # Pick two different random indices\n            idx1 = random.randint(0, len(original_sentences) - 1)\n            idx2 = random.randint(0, len(original_sentences) - 1)\n            \n            if idx1 != idx2:  # Ensure different sentences\n                record = {\n                    'sentences1': original_sentences[idx1],\n                    'sentences2': original_sentences[idx2],\n                    'is_similar': 0\n                }\n                records.append(record)\n                dissimilar_pairs += 1\n            \n            attempts += 1\n        \n        print(f\"  Created {dissimilar_pairs} dissimilar pairs (different originals)\")\n        \n        # Create final DataFrame\n        result_df = pd.DataFrame(records)\n        \n        # Shuffle the pairs\n        result_df = result_df.sample(frac=1, random_state=42).reset_index(drop=True)\n        \n        similar_count = result_df['is_similar'].sum()\n        total_count = len(result_df)\n        \n        print(f\"‚úÖ Generated {total_count} pairs:\")\n        print(f\"  Similar: {similar_count} ({similar_count/total_count:.1%})\")\n        print(f\"  Dissimilar: {total_count - similar_count} ({(total_count - similar_count)/total_count:.1%})\")\n        \n        return result_df\n    \n    def extract_and_convert(self, \n                          table_name: str, \n                          output_path: str,\n                          sample_limit: Optional[int] = None,\n                          matching_mode: str = 'auto',\n                          balance_classes: bool = True,\n                          max_pairs: int = 10000,\n                          augmentations_per_row: int = 3) -> str:\n        \"\"\"\n        Extract data from Hive table and convert to LSTM Siamese format.\n        \n        Args:\n            table_name: Hive table name\n            output_path: Path to save the converted data\n            sample_limit: Limit number of records (None for all)\n            matching_mode: 'auto', 'production', 'testing', or 'cross_product'\n            balance_classes: Whether to balance similar/dissimilar pairs\n            max_pairs: Maximum pairs to generate\n            augmentations_per_row: Number of augmentations per row\n            \n        Returns:\n            Path to the saved file\n        \"\"\"\n        print(f\"üîÑ Extracting data from table: {table_name}\")\n        \n        # Build query\n        if sample_limit:\n            query = f\"SELECT * FROM {table_name} LIMIT {sample_limit}\"\n        else:\n            query = f\"SELECT * FROM {table_name}\"\n        \n        # Execute query\n        df = self.execute_query(query)\n        print(f\"üìä Extracted {len(df)} records\")\n        \n        # Convert to Siamese format\n        print(f\"üîÑ Converting to LSTM Siamese format (mode: {matching_mode})\")\n        if matching_mode == 'cross_product':\n            siamese_df = self.create_cross_product_pairs(df, max_pairs, augmentations_per_row)\n        else:\n            siamese_df = self.create_text_pairs(df, mode=matching_mode)\n        \n        # Balance classes if requested and not already balanced\n        if balance_classes and matching_mode not in ['cross_product']:\n            siamese_df = self.balance_dataset(siamese_df)\n        \n        # Create output directory if needed\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        # Save to CSV\n        siamese_df.to_csv(output_path, index=False)\n        print(f\"üíæ Saved {len(siamese_df)} pairs to: {output_path}\")\n        \n        # Show statistics\n        similar_count = siamese_df['is_similar'].sum()\n        total_count = len(siamese_df)\n        print(f\"üìä Dataset statistics:\")\n        print(f\"  Total pairs: {total_count}\")\n        print(f\"  Similar pairs: {similar_count} ({similar_count/total_count:.1%})\")\n        print(f\"  Dissimilar pairs: {total_count - similar_count} ({(total_count - similar_count)/total_count:.1%})\")\n        \n        return output_path\n    \n    def balance_dataset(self, df: pd.DataFrame, ratio: float = 0.5) -> pd.DataFrame:\n        \"\"\"\n        Balance the dataset to have a specified ratio of similar/dissimilar pairs.\n        \n        Args:\n            df: Input DataFrame with is_similar column\n            ratio: Target ratio of similar pairs (0.5 = balanced)\n            \n        Returns:\n            Balanced DataFrame\n        \"\"\"\n        similar_df = df[df['is_similar'] == 1]\n        dissimilar_df = df[df['is_similar'] == 0]\n        \n        similar_count = len(similar_df)\n        dissimilar_count = len(dissimilar_df)\n        \n        print(f\"üéØ Balancing dataset (current: {similar_count} similar, {dissimilar_count} dissimilar)\")\n        \n        if ratio == 0.5:\n            # Equal balance\n            target_count = min(similar_count, dissimilar_count)\n            if target_count == 0:\n                print(\"‚ö†Ô∏è  Cannot balance - one class has no samples\")\n                return df\n                \n            balanced_similar = similar_df.sample(n=target_count, random_state=42)\n            balanced_dissimilar = dissimilar_df.sample(n=target_count, random_state=42)\n        else:\n            # Custom ratio\n            total_target = min(len(df), max(similar_count, dissimilar_count) * 2)\n            similar_target = int(total_target * ratio)\n            dissimilar_target = total_target - similar_target\n            \n            balanced_similar = similar_df.sample(n=min(similar_target, similar_count), \n                                               random_state=42, replace=similar_target > similar_count)\n            balanced_dissimilar = dissimilar_df.sample(n=min(dissimilar_target, dissimilar_count), \n                                                     random_state=42, replace=dissimilar_target > dissimilar_count)\n        \n        balanced_df = pd.concat([balanced_similar, balanced_dissimilar], ignore_index=True)\n        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n        \n        print(f\"‚úÖ Balanced dataset: {len(balanced_df)} pairs \"\n              f\"({balanced_df['is_similar'].sum()} similar, {len(balanced_df) - balanced_df['is_similar'].sum()} dissimilar)\")\n        \n        return balanced_df"
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Extract training data from Hive\nprint(\"üîÑ Extracting training data from Hive...\")\n\ntry:    \n    extractor = HiveSiameseDataExtractor(\n        host=HIVE_CONFIG['host'],\n        port=HIVE_CONFIG['port'],\n        username=HIVE_CONFIG['username'],\n        database=HIVE_CONFIG['database']\n    )\n    \n    if extractor.connect():\n        print(\"‚úÖ Connected to Hive successfully\")\n        \n        # Extract data with cross-product mode for training\n        training_data_path = extractor.extract_and_convert(\n            table_name=DATA_CONFIG['input_table'],\n            output_path=DATA_CONFIG['training_data_path'],\n            sample_limit=DATA_CONFIG['sample_size'],\n            matching_mode=DATA_CONFIG['matching_mode'],\n            balance_classes=True,\n            max_pairs=10000,  # Generate up to 10k pairs\n            augmentations_per_row=3  # 3 augmented versions per row\n        )\n        \n        print(f\"‚úÖ Training data extracted to: {training_data_path}\")\n        extractor.disconnect()\n        \n        # Load the generated training data\n        training_df = pd.read_csv(training_data_path)\n        \n    else:\n        raise Exception(\"Failed to connect to Hive\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error extracting from Hive: {e}\")\n    print(\"üí° Creating synthetic training data for demonstration...\")\n    \n    # Create synthetic training data with natural language format\n    similar_pairs = [\n        (\"my name is John Smith and my company is Microsoft\", \"my name is Jon Smith and my company is Microsoft Corp\", 1),\n        (\"my name is Mary Johnson and my job is teacher\", \"my name is Maria Johnson and my job is teacher\", 1),\n        (\"my company is Apple Inc and my type is tech company\", \"my company is Apple Incorporated and my type is technology firm\", 1),\n        (\"my weather is sunny and my time is today\", \"my weather is bright and my time is today\", 1),\n        (\"my language is Python and my type is programming\", \"my language is Python and my type is programming language\", 1),\n        (\"my field is machine learning and my method is algorithms\", \"my field is ML and my method is algorithmic approaches\", 1),\n        (\"my city is Barcelona and my country is Spain\", \"my city is Barcelona and my country is Spanish territory\", 1),\n        (\"my business is coffee shop and my service is beverages\", \"my business is caf√© and my service is drinks\", 1),\n        (\"my vehicle is electric car and my feature is eco-friendly\", \"my vehicle is electric and my feature is environmentally friendly\", 1),\n        (\"my field is data science and my tool is statistics\", \"my field is data science and my tool is statistical methods\", 1)\n    ]\n    \n    dissimilar_pairs = [\n        (\"my animal is dog and my trait is loyal\", \"my vehicle is car and my need is maintenance\", 0),\n        (\"my season is summer and my activity is vacation\", \"my season is winter and my activity is sports\", 0),\n        (\"my subject is mathematics and my level is challenging\", \"my subject is history and my level is fascinating\", 0),\n        (\"my food is pizza and my quality is delicious\", \"my device is computer and my feature is powerful\", 0),\n        (\"my feature is mountains and my height is tall\", \"my feature is oceans and my depth is deep\", 0),\n        (\"my item is books and my content is knowledge\", \"my item is music and my effect is joy\", 0),\n        (\"my activity is exercise and my benefit is health\", \"my activity is art and my purpose is creativity\", 0),\n        (\"my field is technology and my pace is rapid\", \"my field is nature and my quality is constant\", 0),\n        (\"my process is learning and my requirement is practice\", \"my outcome is success and my requirement is effort\", 0),\n        (\"my place is cities and my characteristic is crowded\", \"my place is villages and my characteristic is peaceful\", 0)\n    ]\n    \n    # Expand synthetic data\n    all_pairs = []\n    for _ in range(100):  # Replicate to create more training data\n        all_pairs.extend(similar_pairs)\n        all_pairs.extend(dissimilar_pairs)\n    \n    # Create DataFrame\n    training_df = pd.DataFrame(all_pairs, columns=['sentences1', 'sentences2', 'is_similar'])\n    \n    # Shuffle the data\n    training_df = training_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    # Save synthetic data\n    training_df.to_csv(DATA_CONFIG['training_data_path'], index=False)\n    print(f\"‚úÖ Synthetic training data created: {DATA_CONFIG['training_data_path']}\")\n\nprint(f\"\\nüìä Training Data Summary:\")\nprint(f\"  Total pairs: {len(training_df)}\")\nprint(f\"  Similar pairs: {training_df['is_similar'].sum()} ({training_df['is_similar'].mean():.1%})\")\nprint(f\"  Dissimilar pairs: {len(training_df) - training_df['is_similar'].sum()} ({(1-training_df['is_similar'].mean()):.1%})\")\n\n# Display sample data\nprint(f\"\\nüìã Sample Training Data:\")\ndisplay(training_df.head(10))\n\nprint(\"‚úÖ Data extraction completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Perform EDA on training data\nprint(\"üìä Performing Exploratory Data Analysis...\")\n\n# Text length analysis\ntraining_df['len_sentences1'] = training_df['sentences1'].str.len()\ntraining_df['len_sentences2'] = training_df['sentences2'].str.len()\ntraining_df['word_count_sentences1'] = training_df['sentences1'].str.split().str.len()\ntraining_df['word_count_sentences2'] = training_df['sentences2'].str.split().str.len()\n\n# Length difference analysis (create this before filtering)\ntraining_df['len_diff'] = abs(training_df['len_sentences1'] - training_df['len_sentences2'])\n\n# Create filtered DataFrames after all columns are created\nsimilar_df = training_df[training_df['is_similar'] == 1]\ndissimilar_df = training_df[training_df['is_similar'] == 0]\n\n# Create visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Class distribution\naxes[0,0].pie(training_df['is_similar'].value_counts(), \n              labels=['Dissimilar', 'Similar'], \n              autopct='%1.1f%%', \n              startangle=90,\n              colors=['lightcoral', 'lightblue'])\naxes[0,0].set_title('Class Distribution')\n\n# Character length distribution\naxes[0,1].hist(training_df['len_sentences1'], bins=30, alpha=0.7, label='Sentence 1', color='blue')\naxes[0,1].hist(training_df['len_sentences2'], bins=30, alpha=0.7, label='Sentence 2', color='red')\naxes[0,1].set_xlabel('Character Length')\naxes[0,1].set_ylabel('Frequency')\naxes[0,1].set_title('Character Length Distribution')\naxes[0,1].legend()\n\n# Word count distribution\naxes[0,2].hist(training_df['word_count_sentences1'], bins=20, alpha=0.7, label='Sentence 1', color='blue')\naxes[0,2].hist(training_df['word_count_sentences2'], bins=20, alpha=0.7, label='Sentence 2', color='red')\naxes[0,2].set_xlabel('Word Count')\naxes[0,2].set_ylabel('Frequency')\naxes[0,2].set_title('Word Count Distribution')\naxes[0,2].legend()\n\n# Length comparison by similarity (only if we have both classes)\nif len(similar_df) > 0 and len(dissimilar_df) > 0:\n    axes[1,0].boxplot([similar_df['len_sentences1'], dissimilar_df['len_sentences1']], \n                      labels=['Similar', 'Dissimilar'])\n    axes[1,0].set_title('Sentence 1 Length by Similarity')\n    axes[1,0].set_ylabel('Character Length')\n\n    axes[1,1].boxplot([similar_df['word_count_sentences1'], dissimilar_df['word_count_sentences1']], \n                      labels=['Similar', 'Dissimilar'])\n    axes[1,1].set_title('Sentence 1 Word Count by Similarity')\n    axes[1,1].set_ylabel('Word Count')\n\n    # Length difference analysis\n    axes[1,2].hist(similar_df['len_diff'], bins=20, alpha=0.7, label='Similar', color='green')\n    axes[1,2].hist(dissimilar_df['len_diff'], bins=20, alpha=0.7, label='Dissimilar', color='orange')\n    axes[1,2].set_xlabel('Length Difference')\n    axes[1,2].set_ylabel('Frequency')\n    axes[1,2].set_title('Length Difference by Similarity')\n    axes[1,2].legend()\nelse:\n    # Handle case where we don't have both classes\n    axes[1,0].text(0.5, 0.5, 'Insufficient data\\nfor comparison', ha='center', va='center', transform=axes[1,0].transAxes)\n    axes[1,0].set_title('Sentence 1 Length by Similarity')\n    \n    axes[1,1].text(0.5, 0.5, 'Insufficient data\\nfor comparison', ha='center', va='center', transform=axes[1,1].transAxes)\n    axes[1,1].set_title('Sentence 1 Word Count by Similarity')\n    \n    axes[1,2].hist(training_df['len_diff'], bins=20, alpha=0.7, label='All pairs', color='gray')\n    axes[1,2].set_xlabel('Length Difference')\n    axes[1,2].set_ylabel('Frequency')\n    axes[1,2].set_title('Length Difference Distribution')\n    axes[1,2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"\\nüìà Text Statistics:\")\nprint(f\"  Average character length - Sentence 1: {training_df['len_sentences1'].mean():.1f}\")\nprint(f\"  Average character length - Sentence 2: {training_df['len_sentences2'].mean():.1f}\")\nprint(f\"  Average word count - Sentence 1: {training_df['word_count_sentences1'].mean():.1f}\")\nprint(f\"  Average word count - Sentence 2: {training_df['word_count_sentences2'].mean():.1f}\")\nprint(f\"  Max sequence length needed: {max(training_df['word_count_sentences1'].max(), training_df['word_count_sentences2'].max())}\")\n\n# Recommend sequence length\npercentile_95 = np.percentile(np.concatenate([training_df['word_count_sentences1'], training_df['word_count_sentences2']]), 95)\nprint(f\"  Recommended max_sequence_length (95th percentile): {int(percentile_95)}\")\n\n# Class-specific statistics if available\nif len(similar_df) > 0 and len(dissimilar_df) > 0:\n    print(f\"\\nüìä Class-specific Statistics:\")\n    print(f\"  Similar pairs:\")\n    print(f\"    Average length difference: {similar_df['len_diff'].mean():.1f}\")\n    print(f\"    Average sentence 1 length: {similar_df['len_sentences1'].mean():.1f}\")\n    print(f\"  Dissimilar pairs:\")\n    print(f\"    Average length difference: {dissimilar_df['len_diff'].mean():.1f}\")\n    print(f\"    Average sentence 1 length: {dissimilar_df['len_sentences1'].mean():.1f}\")\n\nprint(\"‚úÖ EDA completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Preprocessing text data...\n",
      "üìä Data prepared: 2 sentence pairs\n",
      "üî§ Creating tokenizer...\n",
      "üìñ Vocabulary size: 72\n",
      "\n",
      "üìã Most common words:\n",
      "  num: 1\n",
      "  registre: 2\n",
      "  commerce: 3\n",
      "  nom: 4\n",
      "  prenom: 5\n",
      "  rs: 6\n",
      "  centre: 7\n",
      "  adherent: 8\n",
      "  jamal: 9\n",
      "  assahraa: 10\n",
      "  primary: 11\n",
      "  key: 12\n",
      "  source: 13\n",
      "  table: 14\n",
      "  preprocessed: 15\n",
      "  cleaned: 16\n",
      "  rapprochement: 17\n",
      "  contribuable: 18\n",
      "  derniere: 19\n",
      "  situation: 20\n",
      "\n",
      "üî¢ Converting texts to sequences...\n",
      "üìä Sequence length statistics:\n",
      "  Mean: 66.2\n",
      "  Median: 67.0\n",
      "  95th percentile: 67.0\n",
      "  Max: 67\n",
      "\n",
      "üìè Padding sequences to length 100...\n",
      "‚úÖ Sequences padded: (2, 100), (2, 100)\n",
      "\n",
      "üìã Preprocessing Example:\n",
      "Original text 1: 'primary_key: 678604863659 source_table: preprocessed_cleaned_rapprochement.contribuable_derniere_situation_clean ifu: 2860014 raison_sociale: JAMAL ASSAHRAA nom_prenom_rs: JAMAL ASSAHRAA acronym_nom_prenom_rs: JA adresse: 35 RUE MOULAY ISMAIL DERB BALADIA ice: 79488000046 num_cnss: 6463418 centre_registre_commerce: CASABLANCA code_centre_registre_commerce: 81 num_registre_commerce: 120217 num_article_patente: 33428294 email_adherent: SOC.JMLSAHRA@GMAIL.COM num_tel_adherent: 0522447801'\n",
      "Original text 2: 'primary_key: 841813593481 source_table: preprocessed_cleaned_rapprochement.contribuable_derniere_situation_clean ifu: 40249782 raison_sociale: ORIENT ENVOI nom_prenom_rs: ORIENT ENVOI acronym_nom_prenom_rs: OE adresse: RUE ABDELKARIM EL KHATTABI N 100 102 ice: 1442939000018 centre_registre_commerce: NADOR code_centre_registre_commerce: 49 num_registre_commerce: 9521 num_article_patente: 56126063 email_adherent: ZIZAWA372@GMAIL.COM num_tel_adherent: 0536334484'\n",
      "Tokenized 1: [11, 12, 36, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 37, 23, 24, 9, 10, 4, 5, 6, 9, 10, 25, 4, 5, 6, 38, 26, 39, 27, 40, 41, 42, 43, 28, 44, 1, 45, 46, 7, 2, 3, 47, 29, 7, 2, 3, 48, 1, 2, 3, 49, 1, 30, 31, 50, 32, 8, 51, 52, 33, 34, 1, 35, 8, 53]\n",
      "Tokenized 2: [11, 12, 56, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 57, 23, 24, 54, 55, 4, 5, 6, 54, 55, 25, 4, 5, 6, 58, 26, 27, 59, 60, 61, 62, 63, 64, 28, 65, 7, 2, 3, 66, 29, 7, 2, 3, 67, 1, 2, 3, 68, 1, 30, 31, 69, 32, 8, 70, 33, 34, 1, 35, 8, 71]\n",
      "Padded 1: [11 12 36 13 14 15 16 17 18 19 20 21 22 37 23 24  9 10  4  5]... (showing first 20)\n",
      "Padded 2: [11 12 56 13 14 15 16 17 18 19 20 21 22 57 23 24 54 55  4  5]... (showing first 20)\n",
      "Label: 0\n",
      "‚úÖ Text preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing and tokenization\n",
    "print(\"üî§ Preprocessing text data...\")\n",
    "\n",
    "# Extract sentences and labels\n",
    "sentences1 = training_df['sentences1'].tolist()\n",
    "sentences2 = training_df['sentences2'].tolist()\n",
    "labels = training_df['is_similar'].tolist()\n",
    "\n",
    "print(f\"üìä Data prepared: {len(sentences1)} sentence pairs\")\n",
    "\n",
    "# Create and fit tokenizer\n",
    "print(\"üî§ Creating tokenizer...\")\n",
    "all_sentences = sentences1 + sentences2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"üìñ Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Show most common words\n",
    "word_freq = sorted(tokenizer.word_index.items(), key=lambda x: x[1])\n",
    "print(f\"\\nüìã Most common words:\")\n",
    "for word, idx in word_freq[:20]:\n",
    "    print(f\"  {word}: {idx}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "print(\"\\nüî¢ Converting texts to sequences...\")\n",
    "seq1 = tokenizer.texts_to_sequences(sentences1)\n",
    "seq2 = tokenizer.texts_to_sequences(sentences2)\n",
    "\n",
    "# Analyze sequence lengths\n",
    "seq_lengths = [len(seq) for seq in seq1 + seq2]\n",
    "print(f\"üìä Sequence length statistics:\")\n",
    "print(f\"  Mean: {np.mean(seq_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(seq_lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(seq_lengths, 95):.1f}\")\n",
    "print(f\"  Max: {max(seq_lengths)}\")\n",
    "\n",
    "# Pad sequences\n",
    "max_len = MODEL_CONFIG['max_sequence_length']\n",
    "print(f\"\\nüìè Padding sequences to length {max_len}...\")\n",
    "\n",
    "seq1_padded = pad_sequences(seq1, maxlen=max_len, padding='post', truncating='post')\n",
    "seq2_padded = pad_sequences(seq2, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(f\"‚úÖ Sequences padded: {seq1_padded.shape}, {seq2_padded.shape}\")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Show example of preprocessing\n",
    "print(f\"\\nüìã Preprocessing Example:\")\n",
    "idx = 0\n",
    "print(f\"Original text 1: '{sentences1[idx]}'\")\n",
    "print(f\"Original text 2: '{sentences2[idx]}'\")\n",
    "print(f\"Tokenized 1: {seq1[idx]}\")\n",
    "print(f\"Tokenized 2: {seq2[idx]}\")\n",
    "print(f\"Padded 1: {seq1_padded[idx][:20]}... (showing first 20)\")\n",
    "print(f\"Padded 2: {seq2_padded[idx][:20]}... (showing first 20)\")\n",
    "print(f\"Label: {labels_array[idx]}\")\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Split data into train, validation, and test sets\nprint(\"üîÄ Splitting data into train/validation/test sets...\")\n\n# Check if we have enough data for proper splitting\nunique_labels, label_counts = np.unique(labels_array, return_counts=True)\nmin_samples_per_class = min(label_counts)\ntotal_samples = len(labels_array)\n\nprint(f\"üìä Class distribution:\")\nfor label, count in zip(unique_labels, label_counts):\n    print(f\"  Class {label}: {count} samples\")\nprint(f\"  Total samples: {total_samples}\")\n\n# For very small datasets (< 6 samples), use all data for training and create minimal splits\nif total_samples < 6:\n    print(\"‚ö†Ô∏è  Very small dataset detected. Using simplified splitting strategy.\")\n    \n    # Use most data for training, minimal for validation, and test\n    if total_samples == 2:\n        # With only 2 samples, use 1 for training, 1 for validation, duplicate 1 for test\n        X1_train = seq1_padded[:1]\n        X2_train = seq2_padded[:1]\n        y_train = labels_array[:1]\n        \n        X1_val = seq1_padded[1:2]\n        X2_val = seq2_padded[1:2]\n        y_val = labels_array[1:2]\n        \n        # Use the same sample for test (not ideal but necessary for demo)\n        X1_test = seq1_padded[1:2]\n        X2_test = seq2_padded[1:2]\n        y_test = labels_array[1:2]\n        \n    elif total_samples == 3:\n        # With 3 samples, use 1 for training, 1 for validation, 1 for test\n        X1_train = seq1_padded[:1]\n        X2_train = seq2_padded[:1]\n        y_train = labels_array[:1]\n        \n        X1_val = seq1_padded[1:2]\n        X2_val = seq2_padded[1:2]\n        y_val = labels_array[1:2]\n        \n        X1_test = seq1_padded[2:3]\n        X2_test = seq2_padded[2:3]\n        y_test = labels_array[2:3]\n        \n    else:  # 4 or 5 samples\n        # Use simple train/val/test split\n        X1_train = seq1_padded[:2]\n        X2_train = seq2_padded[:2]\n        y_train = labels_array[:2]\n        \n        if total_samples >= 4:\n            X1_val = seq1_padded[2:3]\n            X2_val = seq2_padded[2:3]\n            y_val = labels_array[2:3]\n            \n            X1_test = seq1_padded[3:]\n            X2_test = seq2_padded[3:]\n            y_test = labels_array[3:]\n        else:\n            # Only 4 samples total\n            X1_val = seq1_padded[2:3]\n            X2_val = seq2_padded[2:3]\n            y_val = labels_array[2:3]\n            \n            X1_test = seq1_padded[3:4]\n            X2_test = seq2_padded[3:4]\n            y_test = labels_array[3:4]\n\nelse:\n    # Normal splitting for larger datasets\n    test_size = TRAINING_CONFIG['test_split']\n    val_size = TRAINING_CONFIG['validation_split']\n    \n    # If we have too few samples per class, skip stratification\n    if min_samples_per_class < 2:\n        print(\"‚ö†Ô∏è  Too few samples per class for stratified splitting. Using simple random split.\")\n        stratify_param = None\n    else:\n        stratify_param = labels_array\n    \n    # First split: separate test set\n    X1_temp, X1_test, X2_temp, X2_test, y_temp, y_test = train_test_split(\n        seq1_padded, seq2_padded, labels_array,\n        test_size=test_size,\n        random_state=42,\n        stratify=stratify_param\n    )\n    \n    # Second split: separate validation from training\n    # Check if remaining data is sufficient for another split\n    if len(X1_temp) < 2:\n        print(\"‚ö†Ô∏è  Insufficient data for validation split. Using training data for validation.\")\n        X1_train, X1_val = X1_temp, X1_temp\n        X2_train, X2_val = X2_temp, X2_temp\n        y_train, y_val = y_temp, y_temp\n    else:\n        adjusted_val_size = val_size / (1 - test_size)\n        \n        # Ensure we don't create empty splits\n        if adjusted_val_size >= 1.0 or len(X1_temp) * adjusted_val_size < 1:\n            # If validation size would be too large, just use 1 sample for validation\n            X1_train = X1_temp[:-1]\n            X2_train = X2_temp[:-1]\n            y_train = y_temp[:-1]\n            \n            X1_val = X1_temp[-1:]\n            X2_val = X2_temp[-1:]\n            y_val = y_temp[-1:]\n        else:\n            X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n                X1_temp, X2_temp, y_temp,\n                test_size=adjusted_val_size,\n                random_state=42,\n                stratify=None  # Remove stratification for small datasets\n            )\n\nprint(f\"üìä Data split completed:\")\nprint(f\"  Training set: {len(X1_train)} pairs ({len(X1_train)/len(seq1_padded):.1%})\")\nif len(y_train) > 0:\n    print(f\"    - Similar: {y_train.sum()} ({y_train.mean():.1%})\")\n    print(f\"    - Dissimilar: {len(y_train) - y_train.sum()} ({1-y_train.mean():.1%})\")\n\nprint(f\"  Validation set: {len(X1_val)} pairs ({len(X1_val)/len(seq1_padded):.1%})\")\nif len(y_val) > 0:\n    print(f\"    - Similar: {y_val.sum()} ({y_val.mean():.1%})\")\n    print(f\"    - Dissimilar: {len(y_val) - y_val.sum()} ({1-y_val.mean():.1%})\")\n\nprint(f\"  Test set: {len(X1_test)} pairs ({len(X1_test)/len(seq1_padded):.1%})\")\nif len(y_test) > 0:\n    print(f\"    - Similar: {y_test.sum()} ({y_test.mean():.1%})\")\n    print(f\"    - Dissimilar: {len(y_test) - y_test.sum()} ({1-y_test.mean():.1%})\")\n\n# Visualize data split (only if we have data in each split)\nif len(y_train) > 0 and len(y_val) > 0 and len(y_test) > 0:\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    for idx, (y_data, title) in enumerate([(y_train, 'Training'), (y_val, 'Validation'), (y_test, 'Test')]):\n        if len(y_data) > 0:\n            counts = np.bincount(y_data.astype(int))\n            # Handle case where one class might be missing\n            if len(counts) == 1:\n                if y_data[0] == 0:\n                    counts = np.array([counts[0], 0])\n                else:\n                    counts = np.array([0, counts[0]])\n            elif len(counts) == 0:\n                counts = np.array([0, 0])\n            \n            # Only plot if we have non-zero counts\n            if np.sum(counts) > 0:\n                axes[idx].pie(counts, labels=['Dissimilar', 'Similar'], autopct='%1.1f%%', startangle=90)\n            else:\n                axes[idx].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[idx].transAxes)\n            axes[idx].set_title(f'{title} Set\\n({len(y_data)} samples)')\n        else:\n            axes[idx].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[idx].transAxes)\n            axes[idx].set_title(f'{title} Set\\n(0 samples)')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"‚ö†Ô∏è  Skipping visualization due to insufficient data in splits\")\n\nprint(\"‚úÖ Data splitting completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design LSTM Siamese model architecture\n",
    "print(\"üèóÔ∏è  Designing LSTM Siamese model architecture...\")\n",
    "\n",
    "def create_lstm_siamese_model(vocab_size, config):\n",
    "    \"\"\"\n",
    "    Create LSTM Siamese neural network model.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        config: Model configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    embedding_dim = config['embedding_dim']\n",
    "    max_len = config['max_sequence_length']\n",
    "    lstm_units = config['number_lstm']\n",
    "    dropout_lstm = config['rate_drop_lstm']\n",
    "    dense_units = config['number_dense_units']\n",
    "    dropout_dense = config['rate_drop_dense']\n",
    "    activation = config['activation_function']\n",
    "    learning_rate = config['learning_rate']\n",
    "    \n",
    "    print(f\"üìä Model Configuration:\")\n",
    "    print(f\"  Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"  Max sequence length: {max_len}\")\n",
    "    print(f\"  LSTM units: {lstm_units}\")\n",
    "    print(f\"  Dense units: {dense_units}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Input layers\n",
    "    input1 = Input(shape=(max_len,), name='input_sentence1')\n",
    "    input2 = Input(shape=(max_len,), name='input_sentence2')\n",
    "    \n",
    "    # Shared embedding layer\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len,\n",
    "        mask_zero=True,  # Handle padding\n",
    "        name='shared_embedding'\n",
    "    )\n",
    "    \n",
    "    # Shared LSTM layer\n",
    "    lstm = Bidirectional(\n",
    "        LSTM(\n",
    "            lstm_units,\n",
    "            dropout=dropout_lstm,\n",
    "            recurrent_dropout=dropout_lstm,\n",
    "            return_sequences=False  # Only return last output\n",
    "        ),\n",
    "        name='shared_bilstm'\n",
    "    )\n",
    "    \n",
    "    # Process both inputs through shared layers\n",
    "    embed1 = embedding(input1)\n",
    "    embed2 = embedding(input2)\n",
    "    \n",
    "    lstm1 = lstm(embed1)\n",
    "    lstm2 = lstm(embed2)\n",
    "    \n",
    "    # Calculate absolute difference (Manhattan distance)\n",
    "    distance = Lambda(\n",
    "        lambda x: tf.abs(x[0] - x[1]),\n",
    "        name='manhattan_distance'\n",
    "    )([lstm1, lstm2])\n",
    "    \n",
    "    # Optional: Add element-wise multiplication\n",
    "    multiply = Lambda(\n",
    "        lambda x: x[0] * x[1],\n",
    "        name='element_multiply'\n",
    "    )([lstm1, lstm2])\n",
    "    \n",
    "    # Concatenate features\n",
    "    concat = tf.keras.layers.Concatenate(name='feature_concat')([distance, multiply])\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    dense1 = Dense(\n",
    "        dense_units,\n",
    "        activation=activation,\n",
    "        name='dense_layer1'\n",
    "    )(concat)\n",
    "    \n",
    "    dropout1 = Dropout(dropout_dense, name='dropout1')(dense1)\n",
    "    \n",
    "    dense2 = Dense(\n",
    "        dense_units // 2,\n",
    "        activation=activation,\n",
    "        name='dense_layer2'\n",
    "    )(dropout1)\n",
    "    \n",
    "    dropout2 = Dropout(dropout_dense, name='dropout2')(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(\n",
    "        1,\n",
    "        activation='sigmoid',\n",
    "        name='similarity_output'\n",
    "    )(dropout2)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(\n",
    "        inputs=[input1, input2],\n",
    "        outputs=output,\n",
    "        name='LSTM_Siamese_Model'\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_lstm_siamese_model(vocab_size, MODEL_CONFIG)\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "# Save model architecture visualization\n",
    "try:\n",
    "    plot_model(\n",
    "        model,\n",
    "        to_file=os.path.join(OUTPUT_CONFIG['model_dir'], f\"model_architecture_{OUTPUT_CONFIG['timestamp']}.png\"),\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB'\n",
    "    )\n",
    "    print(f\"üìä Model architecture diagram saved\")\n",
    "except:\n",
    "    print(f\"‚ö†Ô∏è  Could not save model diagram (graphviz not installed)\")\n",
    "\n",
    "print(\"‚úÖ Model architecture created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training callbacks and monitoring\n",
    "print(\"‚öôÔ∏è  Setting up training callbacks...\")\n",
    "\n",
    "# Model checkpoint callback\n",
    "model_path = os.path.join(\n",
    "    OUTPUT_CONFIG['model_dir'],\n",
    "    f\"{OUTPUT_CONFIG['model_name']}_{OUTPUT_CONFIG['timestamp']}.h5\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=OUTPUT_CONFIG['save_best_only'],\n",
    "    save_weights_only=OUTPUT_CONFIG['save_weights_only'],\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=TRAINING_CONFIG['early_stopping_patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate callback\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=TRAINING_CONFIG['reduce_lr_factor'],\n",
    "    patience=TRAINING_CONFIG['reduce_lr_patience'],\n",
    "    min_lr=TRAINING_CONFIG['min_learning_rate'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Custom callback to log training progress\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_logs = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch_logs.append(logs)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:  # Log every 5 epochs\n",
    "            print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "            print(f\"  Loss: {logs.get('loss', 0):.4f} | Val Loss: {logs.get('val_loss', 0):.4f}\")\n",
    "            print(f\"  Accuracy: {logs.get('accuracy', 0):.4f} | Val Accuracy: {logs.get('val_accuracy', 0):.4f}\")\n",
    "            print(f\"  Precision: {logs.get('precision', 0):.4f} | Val Precision: {logs.get('val_precision', 0):.4f}\")\n",
    "            print(f\"  Recall: {logs.get('recall', 0):.4f} | Val Recall: {logs.get('val_recall', 0):.4f}\")\n",
    "            print(f\"  AUC: {logs.get('auc', 0):.4f} | Val AUC: {logs.get('val_auc', 0):.4f}\")\n",
    "\n",
    "training_logger = TrainingLogger()\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "    early_stopping_callback,\n",
    "    reduce_lr_callback,\n",
    "    training_logger\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Training callbacks configured:\")\n",
    "print(f\"  Model checkpoint: {model_path}\")\n",
    "print(f\"  Early stopping patience: {TRAINING_CONFIG['early_stopping_patience']} epochs\")\n",
    "print(f\"  Learning rate reduction patience: {TRAINING_CONFIG['reduce_lr_patience']} epochs\")\n",
    "print(f\"  Min learning rate: {TRAINING_CONFIG['min_learning_rate']}\")\n",
    "\n",
    "print(\"‚úÖ Training setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM Siamese model\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(f\"‚è∞ Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X1_train, X2_train],\n",
    "        y_train,\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        validation_data=([X1_val, X2_val], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    training_success = False\n",
    "    history = None\n",
    "\n",
    "# Record training end time\n",
    "training_end_time = datetime.now()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\n‚è∞ Training completed at: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è  Total training time: {training_duration}\")\n",
    "\n",
    "if training_success and history:\n",
    "    print(f\"‚úÖ Training completed successfully!\")\n",
    "    \n",
    "    # Get training history\n",
    "    train_logs = training_logger.epoch_logs\n",
    "    epochs_trained = len(train_logs)\n",
    "    \n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    print(f\"  Epochs trained: {epochs_trained}/{TRAINING_CONFIG['epochs']}\")\n",
    "    print(f\"  Final training loss: {train_logs[-1].get('loss', 0):.4f}\")\n",
    "    print(f\"  Final validation loss: {train_logs[-1].get('val_loss', 0):.4f}\")\n",
    "    print(f\"  Final training accuracy: {train_logs[-1].get('accuracy', 0):.4f}\")\n",
    "    print(f\"  Final validation accuracy: {train_logs[-1].get('val_accuracy', 0):.4f}\")\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmin([log.get('val_loss', float('inf')) for log in train_logs])\n",
    "    best_val_loss = train_logs[best_epoch].get('val_loss', 0)\n",
    "    best_val_acc = train_logs[best_epoch].get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"  Best epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Training failed or was interrupted\")\n",
    "\n",
    "print(\"\\n‚úÖ Training phase completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "if training_success and history:\n",
    "    print(\"üìà Visualizing training history...\")\n",
    "    \n",
    "    # Create training plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "    axes[0,0].set_title('Model Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    axes[0,1].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    axes[0,1].set_title('Model Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Precision\n",
    "    axes[0,2].plot(history.history['precision'], label='Training Precision', color='blue')\n",
    "    axes[0,2].plot(history.history['val_precision'], label='Validation Precision', color='red')\n",
    "    axes[0,2].set_title('Model Precision')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('Precision')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Recall\n",
    "    axes[1,0].plot(history.history['recall'], label='Training Recall', color='blue')\n",
    "    axes[1,0].plot(history.history['val_recall'], label='Validation Recall', color='red')\n",
    "    axes[1,0].set_title('Model Recall')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Recall')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: AUC\n",
    "    axes[1,1].plot(history.history['auc'], label='Training AUC', color='blue')\n",
    "    axes[1,1].plot(history.history['val_auc'], label='Validation AUC', color='red')\n",
    "    axes[1,1].set_title('Model AUC')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('AUC')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Learning Rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1,2].plot(history.history['lr'], label='Learning Rate', color='green')\n",
    "        axes[1,2].set_title('Learning Rate Schedule')\n",
    "        axes[1,2].set_xlabel('Epoch')\n",
    "        axes[1,2].set_ylabel('Learning Rate')\n",
    "        axes[1,2].set_yscale('log')\n",
    "        axes[1,2].legend()\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot training vs validation comparison\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        train_scores = history.history['accuracy']\n",
    "        val_scores = history.history['val_accuracy']\n",
    "        \n",
    "        axes[1,2].plot(epochs, train_scores, 'b-', label='Training')\n",
    "        axes[1,2].plot(epochs, val_scores, 'r-', label='Validation')\n",
    "        axes[1,2].fill_between(epochs, train_scores, val_scores, alpha=0.3, color='gray')\n",
    "        axes[1,2].set_title('Train vs Validation Gap')\n",
    "        axes[1,2].set_xlabel('Epoch')\n",
    "        axes[1,2].set_ylabel('Accuracy')\n",
    "        axes[1,2].legend()\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save training plots\n",
    "    plots_path = os.path.join(OUTPUT_CONFIG['model_dir'], f\"training_plots_{OUTPUT_CONFIG['timestamp']}.png\")\n",
    "    plt.savefig(plots_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Training plots saved to: {plots_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Training analysis\n",
    "    print(f\"\\nüìà Training Analysis:\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    print(f\"  Overfitting analysis:\")\n",
    "    print(f\"    Training accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"    Validation accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"    Gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "    if overfitting_gap > 0.1:\n",
    "        print(f\"    ‚ö†Ô∏è  Significant overfitting detected (gap > 0.1)\")\n",
    "        print(f\"    üí° Consider: increased dropout, regularization, or more data\")\n",
    "    elif overfitting_gap > 0.05:\n",
    "        print(f\"    ‚ö†Ô∏è  Mild overfitting detected (gap > 0.05)\")\n",
    "    else:\n",
    "        print(f\"    ‚úÖ No significant overfitting detected\")\n",
    "    \n",
    "    # Learning curve analysis\n",
    "    loss_improvement = history.history['loss'][0] - history.history['loss'][-1]\n",
    "    val_loss_improvement = history.history['val_loss'][0] - history.history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"\\n  Learning progress:\")\n",
    "    print(f\"    Training loss improvement: {loss_improvement:.4f}\")\n",
    "    print(f\"    Validation loss improvement: {val_loss_improvement:.4f}\")\n",
    "    \n",
    "    if val_loss_improvement < 0:\n",
    "        print(f\"    ‚ö†Ô∏è  Validation loss increased during training\")\n",
    "    elif val_loss_improvement < 0.01:\n",
    "        print(f\"    ‚ö†Ô∏è  Limited improvement in validation loss\")\n",
    "    else:\n",
    "        print(f\"    ‚úÖ Good improvement in validation loss\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No training history available for visualization\")\n",
    "\n",
    "print(\"‚úÖ Training analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "if training_success and os.path.exists(model_path):\n",
    "    print(\"üîç Evaluating trained model...\")\n",
    "    \n",
    "    # Load the best model\n",
    "    best_model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"‚úÖ Best model loaded from: {model_path}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüìä Test Set Evaluation:\")\n",
    "    test_results = best_model.evaluate([X1_test, X2_test], y_test, verbose=0)\n",
    "    \n",
    "    # Extract metrics\n",
    "    test_loss = test_results[0]\n",
    "    test_accuracy = test_results[1]\n",
    "    test_precision = test_results[2]\n",
    "    test_recall = test_results[3]\n",
    "    test_auc = test_results[4]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"  AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Get predictions for detailed analysis\n",
    "    test_predictions = best_model.predict([X1_test, X2_test], verbose=0)\n",
    "    test_predictions_binary = (test_predictions.flatten() > 0.5).astype(int)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, test_predictions_binary)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Dissimilar', 'Similar'], \n",
    "                yticklabels=['Dissimilar', 'Similar'],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # Prediction distribution\n",
    "    axes[1].hist(test_predictions[y_test == 0], bins=30, alpha=0.7, label='Dissimilar', color='red')\n",
    "    axes[1].hist(test_predictions[y_test == 1], bins=30, alpha=0.7, label='Similar', color='blue')\n",
    "    axes[1].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "    axes[1].set_xlabel('Prediction Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Prediction Score Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # ROC-like analysis with threshold variation\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_binary = (test_predictions.flatten() > threshold).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        prec = precision_score(y_test, pred_binary, zero_division=0)\n",
    "        rec = recall_score(y_test, pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_test, pred_binary, zero_division=0)\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    axes[2].plot(thresholds, precisions, label='Precision', color='blue')\n",
    "    axes[2].plot(thresholds, recalls, label='Recall', color='red')\n",
    "    axes[2].plot(thresholds, f1_scores, label='F1 Score', color='green')\n",
    "    axes[2].axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Default Threshold')\n",
    "    axes[2].set_xlabel('Threshold')\n",
    "    axes[2].set_ylabel('Score')\n",
    "    axes[2].set_title('Metrics vs Threshold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save evaluation plots\n",
    "    eval_plots_path = os.path.join(OUTPUT_CONFIG['model_dir'], f\"evaluation_plots_{OUTPUT_CONFIG['timestamp']}.png\")\n",
    "    plt.savefig(eval_plots_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nüìä Evaluation plots saved to: {eval_plots_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    optimal_threshold_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "    optimal_f1 = f1_scores[optimal_threshold_idx]\n",
    "    \n",
    "    print(f\"\\nüéØ Optimal threshold analysis:\")\n",
    "    print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"  F1 score at optimal threshold: {optimal_f1:.4f}\")\n",
    "    print(f\"  Precision at optimal threshold: {precisions[optimal_threshold_idx]:.4f}\")\n",
    "    print(f\"  Recall at optimal threshold: {recalls[optimal_threshold_idx]:.4f}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\nüìã Example Predictions (first 10):\")\n",
    "    for i in range(min(10, len(X1_test))):\n",
    "        # Get original sentences (need to reverse tokenization)\n",
    "        pred_score = test_predictions[i][0]\n",
    "        pred_binary = test_predictions_binary[i]\n",
    "        actual = y_test[i]\n",
    "        \n",
    "        status = \"‚úÖ\" if pred_binary == actual else \"‚ùå\"\n",
    "        print(f\"  {status} Score: {pred_score:.3f} | Pred: {pred_binary} | Actual: {actual}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    report = classification_report(y_test, test_predictions_binary, \n",
    "                                  target_names=['Dissimilar', 'Similar'],\n",
    "                                  digits=4)\n",
    "    print(report)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model available for evaluation\")\n",
    "\n",
    "print(\"‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Export and Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model and create documentation\n",
    "if training_success and os.path.exists(model_path):\n",
    "    print(\"üì¶ Exporting trained model for production use...\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = model_path.replace('.h5', '_tokenizer.pkl')\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f\"üíæ Tokenizer saved to: {tokenizer_path}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = model_path.replace('.h5', '_config.json')\n",
    "    \n",
    "    full_config = {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'data_config': {\n",
    "            'vocab_size': vocab_size,\n",
    "            'max_sequence_length': MODEL_CONFIG['max_sequence_length'],\n",
    "            'training_samples': len(X1_train),\n",
    "            'validation_samples': len(X1_val),\n",
    "            'test_samples': len(X1_test)\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'test_precision': float(test_precision),\n",
    "            'test_recall': float(test_recall),\n",
    "            'test_f1': float(test_f1),\n",
    "            'test_auc': float(test_auc),\n",
    "            'optimal_threshold': float(optimal_threshold)\n",
    "        } if 'test_accuracy' in locals() else {},\n",
    "        'training_metadata': {\n",
    "            'timestamp': OUTPUT_CONFIG['timestamp'],\n",
    "            'training_duration_seconds': training_duration.total_seconds(),\n",
    "            'epochs_trained': len(history.history['loss']) if history else 0,\n",
    "            'final_train_loss': float(history.history['loss'][-1]) if history else 0,\n",
    "            'final_val_loss': float(history.history['val_loss'][-1]) if history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(full_config, f, indent=2)\n",
    "    print(f\"üíæ Model configuration saved to: {config_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    if history:\n",
    "        history_path = model_path.replace('.h5', '_history.json')\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        history_dict = {}\n",
    "        for key, values in history.history.items():\n",
    "            history_dict[key] = [float(val) for val in values]\n",
    "        \n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "        print(f\"üíæ Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Create training report\n",
    "    report_path = model_path.replace('.h5', '_training_report.md')\n",
    "    \n",
    "    report_content = f\"\"\"# LSTM Siamese Model Training Report\n",
    "\n",
    "## Training Summary\n",
    "- **Training Date**: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Training Duration**: {training_duration}\n",
    "- **Model Architecture**: LSTM Siamese Neural Network\n",
    "- **Total Parameters**: {total_params:,}\n",
    "\n",
    "## Data Statistics\n",
    "- **Total Training Samples**: {len(X1_train):,}\n",
    "- **Validation Samples**: {len(X1_val):,}\n",
    "- **Test Samples**: {len(X1_test):,}\n",
    "- **Vocabulary Size**: {vocab_size:,}\n",
    "- **Max Sequence Length**: {MODEL_CONFIG['max_sequence_length']}\n",
    "\n",
    "## Model Configuration\n",
    "- **Embedding Dimension**: {MODEL_CONFIG['embedding_dim']}\n",
    "- **LSTM Units**: {MODEL_CONFIG['number_lstm']}\n",
    "- **Dense Units**: {MODEL_CONFIG['number_dense_units']}\n",
    "- **Dropout Rate**: {MODEL_CONFIG['rate_drop_lstm']}\n",
    "- **Learning Rate**: {MODEL_CONFIG['learning_rate']}\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: {TRAINING_CONFIG['epochs']}\n",
    "- **Batch Size**: {TRAINING_CONFIG['batch_size']}\n",
    "- **Early Stopping Patience**: {TRAINING_CONFIG['early_stopping_patience']}\n",
    "- **Learning Rate Reduction**: Enabled\n",
    "\n",
    "## Performance Metrics\n",
    "\"\"\"\n",
    "    \n",
    "    if 'test_accuracy' in locals():\n",
    "        report_content += f\"\"\"\n",
    "### Test Set Performance\n",
    "- **Accuracy**: {test_accuracy:.4f}\n",
    "- **Precision**: {test_precision:.4f}\n",
    "- **Recall**: {test_recall:.4f}\n",
    "- **F1 Score**: {test_f1:.4f}\n",
    "- **AUC**: {test_auc:.4f}\n",
    "- **Optimal Threshold**: {optimal_threshold:.3f}\n",
    "\n",
    "### Training Progress\n",
    "- **Epochs Trained**: {len(history.history['loss']) if history else 'N/A'}\n",
    "- **Final Training Loss**: {history.history['loss'][-1]:.4f if history else 'N/A'}\n",
    "- **Final Validation Loss**: {history.history['val_loss'][-1]:.4f if history else 'N/A'}\n",
    "- **Best Validation Loss**: {min(history.history['val_loss']) if history else 'N/A'}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "\n",
    "## Model Files\n",
    "- **Model**: `{os.path.basename(model_path)}`\n",
    "- **Tokenizer**: `{os.path.basename(tokenizer_path)}`\n",
    "- **Configuration**: `{os.path.basename(config_path)}`\n",
    "- **Training History**: `{os.path.basename(history_path) if history else 'N/A'}`\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Loading the Model\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('{os.path.basename(model_path)}')\n",
    "\n",
    "# Load tokenizer\n",
    "with open('{os.path.basename(tokenizer_path)}', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "```\n",
    "\n",
    "### Making Predictions\n",
    "```python\n",
    "# Prepare text pairs\n",
    "sentences1 = [\"Your first sentence\"]\n",
    "sentences2 = [\"Your second sentence\"]\n",
    "\n",
    "# Tokenize and pad\n",
    "seq1 = tokenizer.texts_to_sequences(sentences1)\n",
    "seq2 = tokenizer.texts_to_sequences(sentences2)\n",
    "seq1_padded = tf.keras.preprocessing.sequence.pad_sequences(seq1, maxlen={MODEL_CONFIG['max_sequence_length']})\n",
    "seq2_padded = tf.keras.preprocessing.sequence.pad_sequences(seq2, maxlen={MODEL_CONFIG['max_sequence_length']})\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict([seq1_padded, seq2_padded])\n",
    "similarity_scores = predictions.flatten()\n",
    "```\n",
    "\n",
    "## Deployment Notes\n",
    "- Recommended threshold: {optimal_threshold:.3f if 'optimal_threshold' in locals() else '0.5'}\n",
    "- Model is ready for Kubeflow inference pipeline\n",
    "- Ensure all required files are available in the deployment environment\n",
    "\n",
    "---\n",
    "*Report generated automatically on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    print(f\"üìÑ Training report saved to: {report_path}\")\n",
    "    \n",
    "    # Print summary of exported files\n",
    "    print(f\"\\nüì¶ Model Export Summary:\")\n",
    "    exported_files = [\n",
    "        model_path,\n",
    "        tokenizer_path,\n",
    "        config_path,\n",
    "        report_path\n",
    "    ]\n",
    "    \n",
    "    if history:\n",
    "        exported_files.append(history_path)\n",
    "    \n",
    "    for file_path in exported_files:\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  ‚úÖ {os.path.basename(file_path)} ({size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Model successfully trained and exported!\")\n",
    "    print(f\"üìÅ All files saved in: {OUTPUT_CONFIG['model_dir']}\")\n",
    "    print(f\"üöÄ Model is ready for production deployment in Kubeflow pipeline\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model available for export\")\n",
    "\n",
    "print(\"\\n‚úÖ Training notebook completed successfully!\")\n",
    "print(f\"‚è∞ Total execution time: {datetime.now() - training_start_time if 'training_start_time' in locals() else 'N/A'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}