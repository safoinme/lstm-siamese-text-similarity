{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LSTM Siamese Model Training Notebook\n",
    "\n",
    "This notebook focuses on **training** the LSTM Siamese model with detailed steps. The trained model will then be used for inference in the Kubeflow pipeline.\n",
    "\n",
    "## Training Workflow:\n",
    "1. **Data Preparation** - Extract and prepare training data from Hive\n",
    "2. **Exploratory Data Analysis** - Understand data distribution and quality\n",
    "3. **Text Preprocessing** - Clean and tokenize text data\n",
    "4. **Model Architecture Design** - Configure LSTM Siamese architecture\n",
    "5. **Training Process** - Train model with monitoring and validation\n",
    "6. **Model Evaluation** - Assess performance and tune hyperparameters\n",
    "7. **Model Export** - Save trained model for production inference\n",
    "8. **Training Documentation** - Generate training report and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Dropout, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('.')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"  TensorFlow version: {tf.__version__}\")\n",
    "print(f\"  GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"  CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "print(\"‚úÖ Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive Configuration\n",
    "HIVE_CONFIG = {\n",
    "    'host': '172.17.235.21',\n",
    "    'port': 10000,\n",
    "    'database': 'preprocessed_analytics',\n",
    "    'username': 'lhimer'\n",
    "}\n",
    "\n",
    "# Data Configuration\n",
    "DATA_CONFIG = {\n",
    "    'input_table': 'preprocessed_analytics.model_reference',\n",
    "    'temp_dir': './training_data',\n",
    "    'raw_data_path': './training_data/raw_data.csv',\n",
    "    'processed_data_path': './training_data/processed_data.csv',\n",
    "    'training_data_path': './training_data/training_pairs.csv',\n",
    "    'validation_data_path': './training_data/validation_pairs.csv',\n",
    "    'test_data_path': './training_data/test_pairs.csv',\n",
    "    'sample_size': 10000,  # Larger sample for training\n",
    "    'matching_mode': 'cross_product',  # Generate training pairs\n",
    "    'balance_ratio': 0.5  # 50% similar, 50% dissimilar\n",
    "}\n",
    "\n",
    "# Model Architecture Configuration\n",
    "MODEL_CONFIG = {\n",
    "    'embedding_dim': 300,\n",
    "    'max_sequence_length': 100,\n",
    "    'number_lstm': 128,  # Larger for better capacity\n",
    "    'rate_drop_lstm': 0.3,\n",
    "    'number_dense_units': 64,\n",
    "    'activation_function': 'relu',\n",
    "    'rate_drop_dense': 0.3,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'epochs': 50,  # More epochs for thorough training\n",
    "    'batch_size': 128,\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'min_learning_rate': 1e-7\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_CONFIG = {\n",
    "    'model_dir': './trained_models',\n",
    "    'model_name': 'lstm_siamese_model',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'save_best_only': True,\n",
    "    'save_weights_only': False\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [DATA_CONFIG['temp_dir'], OUTPUT_CONFIG['model_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"  Model: LSTM({MODEL_CONFIG['number_lstm']}) + Dense({MODEL_CONFIG['number_dense_units']})\")\n",
    "print(f\"  Training samples: {DATA_CONFIG['sample_size']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  Sequence length: {MODEL_CONFIG['max_sequence_length']}\")\n",
    "print(f\"  Output directory: {OUTPUT_CONFIG['model_dir']}\")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Extraction and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data from Hive\n",
    "print(\"üîÑ Extracting training data from Hive...\")\n",
    "\n",
    "try:\n",
    "    from hive_siamese_data_extractor import HiveSiameseDataExtractor\n",
    "    \n",
    "    extractor = HiveSiameseDataExtractor(\n",
    "        host=HIVE_CONFIG['host'],\n",
    "        port=HIVE_CONFIG['port'],\n",
    "        username=HIVE_CONFIG['username'],\n",
    "        database=HIVE_CONFIG['database']\n",
    "    )\n",
    "    \n",
    "    if extractor.connect():\n",
    "        print(\"‚úÖ Connected to Hive successfully\")\n",
    "        \n",
    "        # Extract data with cross-product mode for training\n",
    "        training_data_path = extractor.extract_and_convert(\n",
    "            table_name=DATA_CONFIG['input_table'],\n",
    "            output_path=DATA_CONFIG['training_data_path'],\n",
    "            sample_limit=DATA_CONFIG['sample_size'],\n",
    "            matching_mode=DATA_CONFIG['matching_mode'],\n",
    "            balance_classes=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Training data extracted to: {training_data_path}\")\n",
    "        extractor.disconnect()\n",
    "        \n",
    "        # Load the generated training data\n",
    "        training_df = pd.read_csv(training_data_path)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Failed to connect to Hive\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting from Hive: {e}\")\n",
    "    print(\"üí° Creating synthetic training data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic training data\n",
    "    similar_pairs = [\n",
    "        (\"John Smith works at Microsoft\", \"Jon Smith employed by Microsoft Corp\", 1),\n",
    "        (\"Mary Johnson is a teacher\", \"Maria Johnson teaches students\", 1),\n",
    "        (\"Apple Inc is a tech company\", \"Apple Incorporated technology firm\", 1),\n",
    "        (\"The weather is sunny today\", \"Today has bright sunshine\", 1),\n",
    "        (\"Python is a programming language\", \"Python programming language is popular\", 1),\n",
    "        (\"Machine learning uses algorithms\", \"ML utilizes algorithmic approaches\", 1),\n",
    "        (\"Barcelona is in Spain\", \"Barcelona located in Spanish territory\", 1),\n",
    "        (\"Coffee shops serve beverages\", \"Caf√© establishments provide drinks\", 1),\n",
    "        (\"Electric cars are eco-friendly\", \"Electric vehicles environmentally friendly\", 1),\n",
    "        (\"Data science involves statistics\", \"Data science uses statistical methods\", 1)\n",
    "    ]\n",
    "    \n",
    "    dissimilar_pairs = [\n",
    "        (\"Dogs are loyal pets\", \"Cars need regular maintenance\", 0),\n",
    "        (\"Summer vacation is relaxing\", \"Winter sports are exciting\", 0),\n",
    "        (\"Mathematics is challenging\", \"History is fascinating\", 0),\n",
    "        (\"Pizza is delicious food\", \"Computers are powerful tools\", 0),\n",
    "        (\"Mountains are tall\", \"Oceans are deep\", 0),\n",
    "        (\"Books contain knowledge\", \"Music brings joy\", 0),\n",
    "        (\"Exercise improves health\", \"Art expresses creativity\", 0),\n",
    "        (\"Technology advances rapidly\", \"Nature remains constant\", 0),\n",
    "        (\"Learning requires practice\", \"Success demands effort\", 0),\n",
    "        (\"Cities are crowded\", \"Villages are peaceful\", 0)\n",
    "    ]\n",
    "    \n",
    "    # Expand synthetic data\n",
    "    all_pairs = []\n",
    "    for _ in range(100):  # Replicate to create more training data\n",
    "        all_pairs.extend(similar_pairs)\n",
    "        all_pairs.extend(dissimilar_pairs)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    training_df = pd.DataFrame(all_pairs, columns=['sentences1', 'sentences2', 'is_similar'])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    training_df = training_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Save synthetic data\n",
    "    training_df.to_csv(DATA_CONFIG['training_data_path'], index=False)\n",
    "    print(f\"‚úÖ Synthetic training data created: {DATA_CONFIG['training_data_path']}\")\n",
    "\n",
    "print(f\"\\nüìä Training Data Summary:\")\n",
    "print(f\"  Total pairs: {len(training_df)}\")\n",
    "print(f\"  Similar pairs: {training_df['is_similar'].sum()} ({training_df['is_similar'].mean():.1%})\")\n",
    "print(f\"  Dissimilar pairs: {len(training_df) - training_df['is_similar'].sum()} ({(1-training_df['is_similar'].mean()):.1%})\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìã Sample Training Data:\")\n",
    "display(training_df.head(10))\n",
    "\n",
    "print(\"‚úÖ Data extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA on training data\n",
    "print(\"üìä Performing Exploratory Data Analysis...\")\n",
    "\n",
    "# Text length analysis\n",
    "training_df['len_sentences1'] = training_df['sentences1'].str.len()\n",
    "training_df['len_sentences2'] = training_df['sentences2'].str.len()\n",
    "training_df['word_count_sentences1'] = training_df['sentences1'].str.split().str.len()\n",
    "training_df['word_count_sentences2'] = training_df['sentences2'].str.split().str.len()\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Class distribution\n",
    "axes[0,0].pie(training_df['is_similar'].value_counts(), \n",
    "              labels=['Dissimilar', 'Similar'], \n",
    "              autopct='%1.1f%%', \n",
    "              startangle=90,\n",
    "              colors=['lightcoral', 'lightblue'])\n",
    "axes[0,0].set_title('Class Distribution')\n",
    "\n",
    "# Character length distribution\n",
    "axes[0,1].hist(training_df['len_sentences1'], bins=30, alpha=0.7, label='Sentence 1', color='blue')\n",
    "axes[0,1].hist(training_df['len_sentences2'], bins=30, alpha=0.7, label='Sentence 2', color='red')\n",
    "axes[0,1].set_xlabel('Character Length')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Character Length Distribution')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[0,2].hist(training_df['word_count_sentences1'], bins=20, alpha=0.7, label='Sentence 1', color='blue')\n",
    "axes[0,2].hist(training_df['word_count_sentences2'], bins=20, alpha=0.7, label='Sentence 2', color='red')\n",
    "axes[0,2].set_xlabel('Word Count')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].set_title('Word Count Distribution')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# Length comparison by similarity\n",
    "similar_df = training_df[training_df['is_similar'] == 1]\n",
    "dissimilar_df = training_df[training_df['is_similar'] == 0]\n",
    "\n",
    "axes[1,0].boxplot([similar_df['len_sentences1'], dissimilar_df['len_sentences1']], \n",
    "                  labels=['Similar', 'Dissimilar'])\n",
    "axes[1,0].set_title('Sentence 1 Length by Similarity')\n",
    "axes[1,0].set_ylabel('Character Length')\n",
    "\n",
    "axes[1,1].boxplot([similar_df['word_count_sentences1'], dissimilar_df['word_count_sentences1']], \n",
    "                  labels=['Similar', 'Dissimilar'])\n",
    "axes[1,1].set_title('Sentence 1 Word Count by Similarity')\n",
    "axes[1,1].set_ylabel('Word Count')\n",
    "\n",
    "# Length difference analysis\n",
    "training_df['len_diff'] = abs(training_df['len_sentences1'] - training_df['len_sentences2'])\n",
    "axes[1,2].hist(similar_df['len_diff'], bins=20, alpha=0.7, label='Similar', color='green')\n",
    "axes[1,2].hist(dissimilar_df['len_diff'], bins=20, alpha=0.7, label='Dissimilar', color='orange')\n",
    "axes[1,2].set_xlabel('Length Difference')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "axes[1,2].set_title('Length Difference by Similarity')\n",
    "axes[1,2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nüìà Text Statistics:\")\n",
    "print(f\"  Average character length - Sentence 1: {training_df['len_sentences1'].mean():.1f}\")\n",
    "print(f\"  Average character length - Sentence 2: {training_df['len_sentences2'].mean():.1f}\")\n",
    "print(f\"  Average word count - Sentence 1: {training_df['word_count_sentences1'].mean():.1f}\")\n",
    "print(f\"  Average word count - Sentence 2: {training_df['word_count_sentences2'].mean():.1f}\")\n",
    "print(f\"  Max sequence length needed: {max(training_df['word_count_sentences1'].max(), training_df['word_count_sentences2'].max())}\")\n",
    "\n",
    "# Recommend sequence length\n",
    "percentile_95 = np.percentile(np.concatenate([training_df['word_count_sentences1'], training_df['word_count_sentences2']]), 95)\n",
    "print(f\"  Recommended max_sequence_length (95th percentile): {int(percentile_95)}\")\n",
    "\n",
    "print(\"‚úÖ EDA completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and tokenization\n",
    "print(\"üî§ Preprocessing text data...\")\n",
    "\n",
    "# Extract sentences and labels\n",
    "sentences1 = training_df['sentences1'].tolist()\n",
    "sentences2 = training_df['sentences2'].tolist()\n",
    "labels = training_df['is_similar'].tolist()\n",
    "\n",
    "print(f\"üìä Data prepared: {len(sentences1)} sentence pairs\")\n",
    "\n",
    "# Create and fit tokenizer\n",
    "print(\"üî§ Creating tokenizer...\")\n",
    "all_sentences = sentences1 + sentences2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"üìñ Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Show most common words\n",
    "word_freq = sorted(tokenizer.word_index.items(), key=lambda x: x[1])\n",
    "print(f\"\\nüìã Most common words:\")\n",
    "for word, idx in word_freq[:20]:\n",
    "    print(f\"  {word}: {idx}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "print(\"\\nüî¢ Converting texts to sequences...\")\n",
    "seq1 = tokenizer.texts_to_sequences(sentences1)\n",
    "seq2 = tokenizer.texts_to_sequences(sentences2)\n",
    "\n",
    "# Analyze sequence lengths\n",
    "seq_lengths = [len(seq) for seq in seq1 + seq2]\n",
    "print(f\"üìä Sequence length statistics:\")\n",
    "print(f\"  Mean: {np.mean(seq_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(seq_lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(seq_lengths, 95):.1f}\")\n",
    "print(f\"  Max: {max(seq_lengths)}\")\n",
    "\n",
    "# Pad sequences\n",
    "max_len = MODEL_CONFIG['max_sequence_length']\n",
    "print(f\"\\nüìè Padding sequences to length {max_len}...\")\n",
    "\n",
    "seq1_padded = pad_sequences(seq1, maxlen=max_len, padding='post', truncating='post')\n",
    "seq2_padded = pad_sequences(seq2, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(f\"‚úÖ Sequences padded: {seq1_padded.shape}, {seq2_padded.shape}\")\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Show example of preprocessing\n",
    "print(f\"\\nüìã Preprocessing Example:\")\n",
    "idx = 0\n",
    "print(f\"Original text 1: '{sentences1[idx]}'\")\n",
    "print(f\"Original text 2: '{sentences2[idx]}'\")\n",
    "print(f\"Tokenized 1: {seq1[idx]}\")\n",
    "print(f\"Tokenized 2: {seq2[idx]}\")\n",
    "print(f\"Padded 1: {seq1_padded[idx][:20]}... (showing first 20)\")\n",
    "print(f\"Padded 2: {seq2_padded[idx][:20]}... (showing first 20)\")\n",
    "print(f\"Label: {labels_array[idx]}\")\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "print(\"üîÄ Splitting data into train/validation/test sets...\")\n",
    "\n",
    "# First split: separate test set\n",
    "test_size = TRAINING_CONFIG['test_split']\n",
    "val_size = TRAINING_CONFIG['validation_split']\n",
    "\n",
    "X1_temp, X1_test, X2_temp, X2_test, y_temp, y_test = train_test_split(\n",
    "    seq1_padded, seq2_padded, labels_array,\n",
    "    test_size=test_size,\n",
    "    random_state=42,\n",
    "    stratify=labels_array\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training\n",
    "adjusted_val_size = val_size / (1 - test_size)  # Adjust for the reduced dataset size\n",
    "\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
    "    X1_temp, X2_temp, y_temp,\n",
    "    test_size=adjusted_val_size,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä Data split completed:\")\n",
    "print(f\"  Training set: {len(X1_train)} pairs ({len(X1_train)/len(seq1_padded):.1%})\")\n",
    "print(f\"    - Similar: {y_train.sum()} ({y_train.mean():.1%})\")\n",
    "print(f\"    - Dissimilar: {len(y_train) - y_train.sum()} ({1-y_train.mean():.1%})\")\n",
    "\n",
    "print(f\"  Validation set: {len(X1_val)} pairs ({len(X1_val)/len(seq1_padded):.1%})\")\n",
    "print(f\"    - Similar: {y_val.sum()} ({y_val.mean():.1%})\")\n",
    "print(f\"    - Dissimilar: {len(y_val) - y_val.sum()} ({1-y_val.mean():.1%})\")\n",
    "\n",
    "print(f\"  Test set: {len(X1_test)} pairs ({len(X1_test)/len(seq1_padded):.1%})\")\n",
    "print(f\"    - Similar: {y_test.sum()} ({y_test.mean():.1%})\")\n",
    "print(f\"    - Dissimilar: {len(y_test) - y_test.sum()} ({1-y_test.mean():.1%})\")\n",
    "\n",
    "# Visualize data split\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (y_data, title) in enumerate([(y_train, 'Training'), (y_val, 'Validation'), (y_test, 'Test')]):\n",
    "    counts = np.bincount(y_data)\n",
    "    axes[idx].pie(counts, labels=['Dissimilar', 'Similar'], autopct='%1.1f%%', startangle=90)\n",
    "    axes[idx].set_title(f'{title} Set\\n({len(y_data)} samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Data splitting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design LSTM Siamese model architecture\n",
    "print(\"üèóÔ∏è  Designing LSTM Siamese model architecture...\")\n",
    "\n",
    "def create_lstm_siamese_model(vocab_size, config):\n",
    "    \"\"\"\n",
    "    Create LSTM Siamese neural network model.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        config: Model configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    embedding_dim = config['embedding_dim']\n",
    "    max_len = config['max_sequence_length']\n",
    "    lstm_units = config['number_lstm']\n",
    "    dropout_lstm = config['rate_drop_lstm']\n",
    "    dense_units = config['number_dense_units']\n",
    "    dropout_dense = config['rate_drop_dense']\n",
    "    activation = config['activation_function']\n",
    "    learning_rate = config['learning_rate']\n",
    "    \n",
    "    print(f\"üìä Model Configuration:\")\n",
    "    print(f\"  Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"  Max sequence length: {max_len}\")\n",
    "    print(f\"  LSTM units: {lstm_units}\")\n",
    "    print(f\"  Dense units: {dense_units}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    \n",
    "    # Input layers\n",
    "    input1 = Input(shape=(max_len,), name='input_sentence1')\n",
    "    input2 = Input(shape=(max_len,), name='input_sentence2')\n",
    "    \n",
    "    # Shared embedding layer\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len,\n",
    "        mask_zero=True,  # Handle padding\n",
    "        name='shared_embedding'\n",
    "    )\n",
    "    \n",
    "    # Shared LSTM layer\n",
    "    lstm = Bidirectional(\n",
    "        LSTM(\n",
    "            lstm_units,\n",
    "            dropout=dropout_lstm,\n",
    "            recurrent_dropout=dropout_lstm,\n",
    "            return_sequences=False  # Only return last output\n",
    "        ),\n",
    "        name='shared_bilstm'\n",
    "    )\n",
    "    \n",
    "    # Process both inputs through shared layers\n",
    "    embed1 = embedding(input1)\n",
    "    embed2 = embedding(input2)\n",
    "    \n",
    "    lstm1 = lstm(embed1)\n",
    "    lstm2 = lstm(embed2)\n",
    "    \n",
    "    # Calculate absolute difference (Manhattan distance)\n",
    "    distance = Lambda(\n",
    "        lambda x: tf.abs(x[0] - x[1]),\n",
    "        name='manhattan_distance'\n",
    "    )([lstm1, lstm2])\n",
    "    \n",
    "    # Optional: Add element-wise multiplication\n",
    "    multiply = Lambda(\n",
    "        lambda x: x[0] * x[1],\n",
    "        name='element_multiply'\n",
    "    )([lstm1, lstm2])\n",
    "    \n",
    "    # Concatenate features\n",
    "    concat = tf.keras.layers.Concatenate(name='feature_concat')([distance, multiply])\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    dense1 = Dense(\n",
    "        dense_units,\n",
    "        activation=activation,\n",
    "        name='dense_layer1'\n",
    "    )(concat)\n",
    "    \n",
    "    dropout1 = Dropout(dropout_dense, name='dropout1')(dense1)\n",
    "    \n",
    "    dense2 = Dense(\n",
    "        dense_units // 2,\n",
    "        activation=activation,\n",
    "        name='dense_layer2'\n",
    "    )(dropout1)\n",
    "    \n",
    "    dropout2 = Dropout(dropout_dense, name='dropout2')(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(\n",
    "        1,\n",
    "        activation='sigmoid',\n",
    "        name='similarity_output'\n",
    "    )(dropout2)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(\n",
    "        inputs=[input1, input2],\n",
    "        outputs=output,\n",
    "        name='LSTM_Siamese_Model'\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_lstm_siamese_model(vocab_size, MODEL_CONFIG)\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "# Save model architecture visualization\n",
    "try:\n",
    "    plot_model(\n",
    "        model,\n",
    "        to_file=os.path.join(OUTPUT_CONFIG['model_dir'], f\"model_architecture_{OUTPUT_CONFIG['timestamp']}.png\"),\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB'\n",
    "    )\n",
    "    print(f\"üìä Model architecture diagram saved\")\nexcept:\n",
    "    print(f\"‚ö†Ô∏è  Could not save model diagram (graphviz not installed)\")\n",
    "\n",
    "print(\"‚úÖ Model architecture created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Training Setup and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training callbacks and monitoring\n",
    "print(\"‚öôÔ∏è  Setting up training callbacks...\")\n",
    "\n",
    "# Model checkpoint callback\n",
    "model_path = os.path.join(\n",
    "    OUTPUT_CONFIG['model_dir'],\n",
    "    f\"{OUTPUT_CONFIG['model_name']}_{OUTPUT_CONFIG['timestamp']}.h5\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=OUTPUT_CONFIG['save_best_only'],\n",
    "    save_weights_only=OUTPUT_CONFIG['save_weights_only'],\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=TRAINING_CONFIG['early_stopping_patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate callback\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=TRAINING_CONFIG['reduce_lr_factor'],\n",
    "    patience=TRAINING_CONFIG['reduce_lr_patience'],\n",
    "    min_lr=TRAINING_CONFIG['min_learning_rate'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Custom callback to log training progress\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_logs = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch_logs.append(logs)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:  # Log every 5 epochs\n",
    "            print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "            print(f\"  Loss: {logs.get('loss', 0):.4f} | Val Loss: {logs.get('val_loss', 0):.4f}\")\n",
    "            print(f\"  Accuracy: {logs.get('accuracy', 0):.4f} | Val Accuracy: {logs.get('val_accuracy', 0):.4f}\")\n",
    "            print(f\"  Precision: {logs.get('precision', 0):.4f} | Val Precision: {logs.get('val_precision', 0):.4f}\")\n",
    "            print(f\"  Recall: {logs.get('recall', 0):.4f} | Val Recall: {logs.get('val_recall', 0):.4f}\")\n",
    "            print(f\"  AUC: {logs.get('auc', 0):.4f} | Val AUC: {logs.get('val_auc', 0):.4f}\")\n",
    "\n",
    "training_logger = TrainingLogger()\n",
    "\n",
    "# Combine all callbacks\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "    early_stopping_callback,\n",
    "    reduce_lr_callback,\n",
    "    training_logger\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Training callbacks configured:\")\n",
    "print(f\"  Model checkpoint: {model_path}\")\n",
    "print(f\"  Early stopping patience: {TRAINING_CONFIG['early_stopping_patience']} epochs\")\n",
    "print(f\"  Learning rate reduction patience: {TRAINING_CONFIG['reduce_lr_patience']} epochs\")\n",
    "print(f\"  Min learning rate: {TRAINING_CONFIG['min_learning_rate']}\")\n",
    "\n",
    "print(\"‚úÖ Training setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM Siamese model\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(f\"‚è∞ Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X1_train, X2_train],\n",
    "        y_train,\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        validation_data=([X1_val, X2_val], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_success = True\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    training_success = False\n",
    "    history = None\n",
    "\n",
    "# Record training end time\n",
    "training_end_time = datetime.now()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\n‚è∞ Training completed at: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è  Total training time: {training_duration}\")\n",
    "\n",
    "if training_success and history:\n",
    "    print(f\"‚úÖ Training completed successfully!\")\n",
    "    \n",
    "    # Get training history\n",
    "    train_logs = training_logger.epoch_logs\n",
    "    epochs_trained = len(train_logs)\n",
    "    \n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    print(f\"  Epochs trained: {epochs_trained}/{TRAINING_CONFIG['epochs']}\")\n",
    "    print(f\"  Final training loss: {train_logs[-1].get('loss', 0):.4f}\")\n",
    "    print(f\"  Final validation loss: {train_logs[-1].get('val_loss', 0):.4f}\")\n",
    "    print(f\"  Final training accuracy: {train_logs[-1].get('accuracy', 0):.4f}\")\n",
    "    print(f\"  Final validation accuracy: {train_logs[-1].get('val_accuracy', 0):.4f}\")\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmin([log.get('val_loss', float('inf')) for log in train_logs])\n",
    "    best_val_loss = train_logs[best_epoch].get('val_loss', 0)\n",
    "    best_val_acc = train_logs[best_epoch].get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"  Best epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Training failed or was interrupted\")\n",
    "\n",
    "print(\"\\n‚úÖ Training phase completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Training Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "if training_success and history:\n",
    "    print(\"üìà Visualizing training history...\")\n",
    "    \n",
    "    # Create training plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "    axes[0,0].set_title('Model Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    axes[0,1].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    axes[0,1].set_title('Model Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Precision\n",
    "    axes[0,2].plot(history.history['precision'], label='Training Precision', color='blue')\n",
    "    axes[0,2].plot(history.history['val_precision'], label='Validation Precision', color='red')\n",
    "    axes[0,2].set_title('Model Precision')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('Precision')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Recall\n",
    "    axes[1,0].plot(history.history['recall'], label='Training Recall', color='blue')\n",
    "    axes[1,0].plot(history.history['val_recall'], label='Validation Recall', color='red')\n",
    "    axes[1,0].set_title('Model Recall')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Recall')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: AUC\n",
    "    axes[1,1].plot(history.history['auc'], label='Training AUC', color='blue')\n",
    "    axes[1,1].plot(history.history['val_auc'], label='Validation AUC', color='red')\n",
    "    axes[1,1].set_title('Model AUC')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('AUC')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Learning Rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1,2].plot(history.history['lr'], label='Learning Rate', color='green')\n",
    "        axes[1,2].set_title('Learning Rate Schedule')\n",
    "        axes[1,2].set_xlabel('Epoch')\n",
    "        axes[1,2].set_ylabel('Learning Rate')\n",
    "        axes[1,2].set_yscale('log')\n",
    "        axes[1,2].legend()\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot training vs validation comparison\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        train_scores = history.history['accuracy']\n",
    "        val_scores = history.history['val_accuracy']\n",
    "        \n",
    "        axes[1,2].plot(epochs, train_scores, 'b-', label='Training')\n",
    "        axes[1,2].plot(epochs, val_scores, 'r-', label='Validation')\n",
    "        axes[1,2].fill_between(epochs, train_scores, val_scores, alpha=0.3, color='gray')\n",
    "        axes[1,2].set_title('Train vs Validation Gap')\n",
    "        axes[1,2].set_xlabel('Epoch')\n",
    "        axes[1,2].set_ylabel('Accuracy')\n",
    "        axes[1,2].legend()\n",
    "        axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save training plots\n",
    "    plots_path = os.path.join(OUTPUT_CONFIG['model_dir'], f\"training_plots_{OUTPUT_CONFIG['timestamp']}.png\")\n",
    "    plt.savefig(plots_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Training plots saved to: {plots_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Training analysis\n",
    "    print(f\"\\nüìà Training Analysis:\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    print(f\"  Overfitting analysis:\")\n",
    "    print(f\"    Training accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"    Validation accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"    Gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "    if overfitting_gap > 0.1:\n",
    "        print(f\"    ‚ö†Ô∏è  Significant overfitting detected (gap > 0.1)\")\n",
    "        print(f\"    üí° Consider: increased dropout, regularization, or more data\")\n",
    "    elif overfitting_gap > 0.05:\n",
    "        print(f\"    ‚ö†Ô∏è  Mild overfitting detected (gap > 0.05)\")\n",
    "    else:\n",
    "        print(f\"    ‚úÖ No significant overfitting detected\")\n",
    "    \n",
    "    # Learning curve analysis\n",
    "    loss_improvement = history.history['loss'][0] - history.history['loss'][-1]\n",
    "    val_loss_improvement = history.history['val_loss'][0] - history.history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"\\n  Learning progress:\")\n",
    "    print(f\"    Training loss improvement: {loss_improvement:.4f}\")\n",
    "    print(f\"    Validation loss improvement: {val_loss_improvement:.4f}\")\n",
    "    \n",
    "    if val_loss_improvement < 0:\n",
    "        print(f\"    ‚ö†Ô∏è  Validation loss increased during training\")\n",
    "    elif val_loss_improvement < 0.01:\n",
    "        print(f\"    ‚ö†Ô∏è  Limited improvement in validation loss\")\n",
    "    else:\n",
    "        print(f\"    ‚úÖ Good improvement in validation loss\")\n",
    "\nelse:\n",
    "    print(\"‚ùå No training history available for visualization\")\n",
    "\nprint(\"‚úÖ Training analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "if training_success and os.path.exists(model_path):\n",
    "    print(\"üîç Evaluating trained model...\")\n",
    "    \n",
    "    # Load the best model\n",
    "    best_model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"‚úÖ Best model loaded from: {model_path}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nüìä Test Set Evaluation:\")\n",
    "    test_results = best_model.evaluate([X1_test, X2_test], y_test, verbose=0)\n",
    "    \n",
    "    # Extract metrics\n",
    "    test_loss = test_results[0]\n",
    "    test_accuracy = test_results[1]\n",
    "    test_precision = test_results[2]\n",
    "    test_recall = test_results[3]\n",
    "    test_auc = test_results[4]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"  Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"  AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    # Get predictions for detailed analysis\n",
    "    test_predictions = best_model.predict([X1_test, X2_test], verbose=0)\n",
    "    test_predictions_binary = (test_predictions.flatten() > 0.5).astype(int)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, test_predictions_binary)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Dissimilar', 'Similar'], \n",
    "                yticklabels=['Dissimilar', 'Similar'],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # Prediction distribution\n",
    "    axes[1].hist(test_predictions[y_test == 0], bins=30, alpha=0.7, label='Dissimilar', color='red')\n",
    "    axes[1].hist(test_predictions[y_test == 1], bins=30, alpha=0.7, label='Similar', color='blue')\n",
    "    axes[1].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "    axes[1].set_xlabel('Prediction Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Prediction Score Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # ROC-like analysis with threshold variation\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_binary = (test_predictions.flatten() > threshold).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        prec = precision_score(y_test, pred_binary, zero_division=0)\n",
    "        rec = recall_score(y_test, pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_test, pred_binary, zero_division=0)\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    axes[2].plot(thresholds, precisions, label='Precision', color='blue')\n",
    "    axes[2].plot(thresholds, recalls, label='Recall', color='red')\n",
    "    axes[2].plot(thresholds, f1_scores, label='F1 Score', color='green')\n",
    "    axes[2].axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Default Threshold')\n",
    "    axes[2].set_xlabel('Threshold')\n",
    "    axes[2].set_ylabel('Score')\n",
    "    axes[2].set_title('Metrics vs Threshold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save evaluation plots\n",
    "    eval_plots_path = os.path.join(OUTPUT_CONFIG['model_dir'], f\"evaluation_plots_{OUTPUT_CONFIG['timestamp']}.png\")\n",
    "    plt.savefig(eval_plots_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nüìä Evaluation plots saved to: {eval_plots_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    optimal_threshold_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "    optimal_f1 = f1_scores[optimal_threshold_idx]\n",
    "    \n",
    "    print(f\"\\nüéØ Optimal threshold analysis:\")\n",
    "    print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"  F1 score at optimal threshold: {optimal_f1:.4f}\")\n",
    "    print(f\"  Precision at optimal threshold: {precisions[optimal_threshold_idx]:.4f}\")\n",
    "    print(f\"  Recall at optimal threshold: {recalls[optimal_threshold_idx]:.4f}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\nüìã Example Predictions (first 10):\")\n",
    "    for i in range(min(10, len(X1_test))):\n",
    "        # Get original sentences (need to reverse tokenization)\n",
    "        pred_score = test_predictions[i][0]\n",
    "        pred_binary = test_predictions_binary[i]\n",
    "        actual = y_test[i]\n",
    "        \n",
    "        status = \"‚úÖ\" if pred_binary == actual else \"‚ùå\"\n",
    "        print(f\"  {status} Score: {pred_score:.3f} | Pred: {pred_binary} | Actual: {actual}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    report = classification_report(y_test, test_predictions_binary, \n",
    "                                  target_names=['Dissimilar', 'Similar'],\n",
    "                                  digits=4)\n",
    "    print(report)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model available for evaluation\")\n",
    "\nprint(\"‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 12. Model Export and Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model and create documentation\n",
    "if training_success and os.path.exists(model_path):\n",
    "    print(\"üì¶ Exporting trained model for production use...\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = model_path.replace('.h5', '_tokenizer.pkl')\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f\"üíæ Tokenizer saved to: {tokenizer_path}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = model_path.replace('.h5', '_config.json')\n",
    "    \n",
    "    full_config = {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'data_config': {\n",
    "            'vocab_size': vocab_size,\n",
    "            'max_sequence_length': MODEL_CONFIG['max_sequence_length'],\n",
    "            'training_samples': len(X1_train),\n",
    "            'validation_samples': len(X1_val),\n",
    "            'test_samples': len(X1_test)\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'test_precision': float(test_precision),\n",
    "            'test_recall': float(test_recall),\n",
    "            'test_f1': float(test_f1),\n",
    "            'test_auc': float(test_auc),\n",
    "            'optimal_threshold': float(optimal_threshold)\n",
    "        } if 'test_accuracy' in locals() else {},\n",
    "        'training_metadata': {\n",
    "            'timestamp': OUTPUT_CONFIG['timestamp'],\n",
    "            'training_duration_seconds': training_duration.total_seconds(),\n",
    "            'epochs_trained': len(history.history['loss']) if history else 0,\n",
    "            'final_train_loss': float(history.history['loss'][-1]) if history else 0,\n",
    "            'final_val_loss': float(history.history['val_loss'][-1]) if history else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(full_config, f, indent=2)\n",
    "    print(f\"üíæ Model configuration saved to: {config_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    if history:\n",
    "        history_path = model_path.replace('.h5', '_history.json')\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        history_dict = {}\n",
    "        for key, values in history.history.items():\n",
    "            history_dict[key] = [float(val) for val in values]\n",
    "        \n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "        print(f\"üíæ Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Create training report\n",
    "    report_path = model_path.replace('.h5', '_training_report.md')\n",
    "    \n",
    "    report_content = f\"\"\"# LSTM Siamese Model Training Report\n",
    "\n",
    "## Training Summary\n",
    "- **Training Date**: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Training Duration**: {training_duration}\n",
    "- **Model Architecture**: LSTM Siamese Neural Network\n",
    "- **Total Parameters**: {total_params:,}\n",
    "\n",
    "## Data Statistics\n",
    "- **Total Training Samples**: {len(X1_train):,}\n",
    "- **Validation Samples**: {len(X1_val):,}\n",
    "- **Test Samples**: {len(X1_test):,}\n",
    "- **Vocabulary Size**: {vocab_size:,}\n",
    "- **Max Sequence Length**: {MODEL_CONFIG['max_sequence_length']}\n",
    "\n",
    "## Model Configuration\n",
    "- **Embedding Dimension**: {MODEL_CONFIG['embedding_dim']}\n",
    "- **LSTM Units**: {MODEL_CONFIG['number_lstm']}\n",
    "- **Dense Units**: {MODEL_CONFIG['number_dense_units']}\n",
    "- **Dropout Rate**: {MODEL_CONFIG['rate_drop_lstm']}\n",
    "- **Learning Rate**: {MODEL_CONFIG['learning_rate']}\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: {TRAINING_CONFIG['epochs']}\n",
    "- **Batch Size**: {TRAINING_CONFIG['batch_size']}\n",
    "- **Early Stopping Patience**: {TRAINING_CONFIG['early_stopping_patience']}\n",
    "- **Learning Rate Reduction**: Enabled\n",
    "\n",
    "## Performance Metrics\n",
    "\"\"\"\n",
    "    \n",
    "    if 'test_accuracy' in locals():\n",
    "        report_content += f\"\"\"\n",
    "### Test Set Performance\n",
    "- **Accuracy**: {test_accuracy:.4f}\n",
    "- **Precision**: {test_precision:.4f}\n",
    "- **Recall**: {test_recall:.4f}\n",
    "- **F1 Score**: {test_f1:.4f}\n",
    "- **AUC**: {test_auc:.4f}\n",
    "- **Optimal Threshold**: {optimal_threshold:.3f}\n",
    "\n",
    "### Training Progress\n",
    "- **Epochs Trained**: {len(history.history['loss']) if history else 'N/A'}\n",
    "- **Final Training Loss**: {history.history['loss'][-1]:.4f if history else 'N/A'}\n",
    "- **Final Validation Loss**: {history.history['val_loss'][-1]:.4f if history else 'N/A'}\n",
    "- **Best Validation Loss**: {min(history.history['val_loss']) if history else 'N/A'}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "\n",
    "## Model Files\n",
    "- **Model**: `{os.path.basename(model_path)}`\n",
    "- **Tokenizer**: `{os.path.basename(tokenizer_path)}`\n",
    "- **Configuration**: `{os.path.basename(config_path)}`\n",
    "- **Training History**: `{os.path.basename(history_path) if history else 'N/A'}`\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Loading the Model\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('{os.path.basename(model_path)}')\n",
    "\n",
    "# Load tokenizer\n",
    "with open('{os.path.basename(tokenizer_path)}', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "```\n",
    "\n",
    "### Making Predictions\n",
    "```python\n",
    "# Prepare text pairs\n",
    "sentences1 = [\"Your first sentence\"]\n",
    "sentences2 = [\"Your second sentence\"]\n",
    "\n",
    "# Tokenize and pad\n",
    "seq1 = tokenizer.texts_to_sequences(sentences1)\n",
    "seq2 = tokenizer.texts_to_sequences(sentences2)\n",
    "seq1_padded = tf.keras.preprocessing.sequence.pad_sequences(seq1, maxlen={MODEL_CONFIG['max_sequence_length']})\n",
    "seq2_padded = tf.keras.preprocessing.sequence.pad_sequences(seq2, maxlen={MODEL_CONFIG['max_sequence_length']})\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict([seq1_padded, seq2_padded])\n",
    "similarity_scores = predictions.flatten()\n",
    "```\n",
    "\n",
    "## Deployment Notes\n",
    "- Recommended threshold: {optimal_threshold:.3f if 'optimal_threshold' in locals() else '0.5'}\n",
    "- Model is ready for Kubeflow inference pipeline\n",
    "- Ensure all required files are available in the deployment environment\n",
    "\n",
    "---\n",
    "*Report generated automatically on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    print(f\"üìÑ Training report saved to: {report_path}\")\n",
    "    \n",
    "    # Print summary of exported files\n",
    "    print(f\"\\nüì¶ Model Export Summary:\")\n",
    "    exported_files = [\n",
    "        model_path,\n",
    "        tokenizer_path,\n",
    "        config_path,\n",
    "        report_path\n",
    "    ]\n",
    "    \n",
    "    if history:\n",
    "        exported_files.append(history_path)\n",
    "    \n",
    "    for file_path in exported_files:\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  ‚úÖ {os.path.basename(file_path)} ({size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Model successfully trained and exported!\")\n",
    "    print(f\"üìÅ All files saved in: {OUTPUT_CONFIG['model_dir']}\")\n",
    "    print(f\"üöÄ Model is ready for production deployment in Kubeflow pipeline\")\n",
    "    \nelse:\n",
    "    print(\"‚ùå No trained model available for export\")\n",
    "\nprint(\"\\n‚úÖ Training notebook completed successfully!\")\nprint(f\"‚è∞ Total execution time: {datetime.now() - training_start_time if 'training_start_time' in locals() else 'N/A'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}